{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0157e3-1b74-4592-8494-86ac3be129e0",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "# Practical 2: Forecasting Housing Prices\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wIgF2_GabxOZ",
   "metadata": {
    "executionInfo": {
     "elapsed": 2466,
     "status": "ok",
     "timestamp": 1695536714957,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "wIgF2_GabxOZ"
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23538915-b8b4-465b-9d6e-e6539f238144",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "## The California Housing data set\n",
    "\n",
    "The \"California Housing\" data set is a widely used data set to demonstrate forecasting methods. As the name suggests, the data set concerns the valuation of real estate. It comprises socio-demographic information concerning the area of a property and a dependent (aka target) variable, which gives the median house value for California districts. This data was derived from the 1990 U.S. census. \n",
    "\n",
    "Being so popular, the data set is readily available in standard Python libraries. Execute the following code to load the data set from `sklearn.datasets` and familiarize yourself with the data description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4cc9d-5bb8-47cc-948a-4ab6413a6189",
   "metadata": {
    "executionInfo": {
     "elapsed": 1738,
     "status": "ok",
     "timestamp": 1695536716691,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "1df4cc9d-5bb8-47cc-948a-4ab6413a6189"
   },
   "outputs": [],
   "source": [
    "# Downloading the data set\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california_housing = fetch_california_housing(as_frame=True)  # get the data as a Pandas dataframe\n",
    "\n",
    "print(california_housing.DESCR)  # the data even comes with a nice description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0489dd6-54fd-4ac2-ae35-6385427131dd",
   "metadata": {},
   "source": [
    "The `sklearn` library provides the data in a specific format. Feature values and the target variable are already separated. The be consistent with our standard notation, we extract the feature values and target and store it, as usual, in variables $X$ and $y$, respectively. Of course, this is a good opportunity to also take a quick look into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf24cd-e9b2-4264-8b99-7f7c1096f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = california_housing.data\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdbb36-d876-473c-9f8d-103e3d2ddad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = california_housing.target\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d88f2f1-c54e-4a69-a159-96a64b3daec6",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada977f-144d-4163-87f6-b47238761c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.describe()  # compute descriptive statistics that summarize the target's distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a3bc8-b53d-43bb-a5b7-0340ac3a4bba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.describe() # do the same for all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a41e3b-0ea4-4697-a921-84b1651ec36d",
   "metadata": {},
   "source": [
    "### Explanatory data analysis (EDA)\n",
    "To better understand the data, we consider a few standard EDA operations. They comprise the analysis of histograms and/or box-plots of the target and the features. Also, we consider the (linear) correlation between features and the target to obtain some initial evidence as to which features might be important. \n",
    "\n",
    "While we only create the plots in the following, never forget that each plot - and more generally every result - deserves a careful analysis and discussion. Therefore, make sure to examine each plot and note down your key observations.\n",
    "\n",
    "Without any strong reason, we use the `seaborn` library for plotting in this notebook. Many data scientists prefer `seaborn` over `Matplotlib`. Hence, we illustrate it here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e420d55-5e3a-474f-b53d-f87c72afcfd7",
   "metadata": {},
   "source": [
    "#### Target Variable: Medium House Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197b8bb7",
   "metadata": {},
   "source": [
    "##### Analyzing the distribution of the target by means of a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WXjFWs3zF31A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1695536838762,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "WXjFWs3zF31A",
    "outputId": "c5473e54-8a6c-4908-f220-f0c7034d04e9"
   },
   "outputs": [],
   "source": [
    "# Histogram of target\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(y, bins=30, kde=False, edgecolor='k')\n",
    "plt.xlabel(y.name)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Distribution of {y.name}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa9d825",
   "metadata": {},
   "source": [
    "##### Analyzing the distribution of the target by means of a boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e5a28-cf93-4181-b516-779616999f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=y, color='skyblue');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618de746-2510-4f06-a083-865162740f15",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xdXLDm-OF4Sn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "executionInfo": {
     "elapsed": 1210,
     "status": "ok",
     "timestamp": 1695536812007,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "xdXLDm-OF4Sn",
    "outputId": "745d4107-6eed-431a-c3fa-1ed56aea2b05"
   },
   "outputs": [],
   "source": [
    "#3x3 matrix of box-plots for all the features\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))  # Create a 3x3 grid of subplots\n",
    "axes = axes.flatten() # Flatten the axes for easier iteration\n",
    "\n",
    "# Loop through each feature and create a box plot\n",
    "for i, feature in enumerate(X.columns):\n",
    "    sns.boxplot(x=X[feature], ax=axes[i], color='skyblue')\n",
    "    axes[i].set_title(f'Box Plot of {feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(X.columns), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d3ad5-13fa-45a2-8226-79c3aeee595e",
   "metadata": {},
   "source": [
    "#### Correlation analysis\n",
    "Having obtained an idea about the *univariate* distributions, we also take a look at the correlation structure in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e09df1-2978-4e25-a0d9-4a06037729fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1695536838139,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "e_t4YznoF4AF",
    "outputId": "9e7d58d7-47c6-4418-db79-24b74d2f2afb"
   },
   "outputs": [],
   "source": [
    "# Correlation among the features\n",
    "corr_matrix = X.corr()  \n",
    "sns.heatmap(corr_matrix, cmap='vlag', annot=True)\n",
    "plt.title('Feature correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7cc71-9e52-4f7f-905f-e769468987c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between individual features and the target\n",
    "rho = X.corrwith(y)\n",
    "print(rho)\n",
    "\n",
    "sns.barplot(x=rho, y=X.columns.tolist(), orient='h')\n",
    "plt.title('Feature to target correlation')\n",
    "plt.xlabel('Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47300a-b43f-4482-a20a-034d18f92a0c",
   "metadata": {
    "id": "64fizQ6g6rQg"
   },
   "source": [
    "## Linear Regression\n",
    "Having completed our initial data screening and explorative analysis, we proceed by estimating a linear regression model to deepen our understanding of how the features and the target are related to another.\n",
    "\n",
    "To that end, we consider the library `statsmodels`, a popular and powerful library for statistical modeling, which includes ordinary least squares (OLS) estimator. We provide all codes for the model estimation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e353f01-f945-4734-bbf5-22ac905d10f4",
   "metadata": {
    "executionInfo": {
     "elapsed": 664,
     "status": "ok",
     "timestamp": 1695536839415,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "edVKGWXfgWtf"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm \n",
    "\n",
    "# OLS model estimation\n",
    "lr_sm = sm.OLS(y,\n",
    "               sm.add_constant(X)  # include an intercept\n",
    "              ) \n",
    "\n",
    "lr_sm = lr_sm.fit()  # fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a9c41-e584-455f-8081-4f4d86a55631",
   "metadata": {
    "id": "64fizQ6g6rQg"
   },
   "source": [
    "The library provides a neat function, `.summary()` to obtain a concise overview of the results of regression analysis. It includes key information like R-squared, estimated coefficients, standard errors, and p-values. This summary is crucial for evaluating model adequacy and feature significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d635c-b74b-4dde-9c41-1ebe06c9c392",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1695536839415,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "7GQ935p6WhxW",
    "outputId": "d0a8fb32-c2a8-41b6-8141-fc82fb6402ca"
   },
   "outputs": [],
   "source": [
    "print(lr_sm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dbf141-f21e-4c9e-9c94-51b07f1f634e",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "# Exercises\n",
    "Based on the above analysis, draw on your data science expertise to answer the following questions: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4373d5-09a1-430d-a6c2-41ea75601c3b",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "## 1. What are the two most and the two least important features?\n",
    "Briefly note what statistics/results you have considered to make your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2714140-c525-4909-abca-c0b03412ad9d",
   "metadata": {},
   "source": [
    "**Enter your answer in this markup cell:**\n",
    "\n",
    "Suitable statistics to clarify the importance of individual features are the T-Test and the feature-to-target correlation.\n",
    "\n",
    "Considering the **T-test**, the two most important features are:\n",
    "- MedInc\n",
    "- Latitude\n",
    "\n",
    "and the two least important features are:\n",
    "- Population\n",
    "- AveOccup\n",
    "\n",
    "Considering the **Feature-to-target correlation**, the two most important features are:\n",
    "- MedInc\n",
    "- AveRooms\n",
    "\n",
    "and the two least important features are:\n",
    "- Population\n",
    "- AveOccup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c03ef4-6116-4ae0-8118-0f3f1d0e9087",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "## 2. Scatter Plots\n",
    "Create, for each of the selected features, a scatter plot of the selected feature against the target. Display these plots in a 2x2 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0facc228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for plotting \n",
    "features_to_plot = ['MedInc', 'AveRooms', 'Population', 'AveOccup']  # Selection based on correlation\n",
    "\n",
    "# Create a 2x2 grid of scatter plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "fig.suptitle(\"Scatter Plots of Most/Least Important Features vs. Y\", fontsize=16)\n",
    "\n",
    "# Iterate through features and plot using Seaborn\n",
    "for i, column in enumerate(features_to_plot):\n",
    "    row, col = divmod(i, 2)\n",
    "    sns.scatterplot(x=X[column], y=y, ax=axes[row, col], alpha=0.7)\n",
    "    axes[row, col].set_title(f\"{column} vs Y\")\n",
    "    axes[row, col].set_xlabel(column)\n",
    "    axes[row, col].set_ylabel(\"Y\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit title\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2445b153-5b13-4903-9c38-a97aa6f27a09",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "## 3. Model Reestimation\n",
    "Remove the two least important features from the data and reestimate the model. Briefly discuss whether this step has improved the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908535cb-275a-49a5-9e11-772144f60b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to reestimate the model:\n",
    "\n",
    "# Remove least important features (based on correlation)\n",
    "X_sparse = X.drop(columns=['AveOccup', 'Population'])\n",
    "\n",
    "# Reestimate the model\n",
    "sparse_model = sm.OLS(y,\n",
    "               sm.add_constant(X_sparse)  # include an intercept\n",
    "              ) \n",
    "\n",
    "sparse_results = sparse_model.fit()  # fit the modelX_sparse\n",
    "\n",
    "print(sparse_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa95dc0-f8f9-4084-95cb-39cb60e4dbf5",
   "metadata": {},
   "source": [
    "**Did the model improve? Briefly discuss:**\n",
    "\n",
    "Overall, differences between the models are minor. We could, for example, compare the $R^2$ statistics as shown below. Of course, you can also consider other statistics such as, e.g.,  *AIC*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b19d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R2 when using all features: {lr_sm.rsquared_adj:.5f}')\n",
    "print(f'R2 when discarding two least important features: {sparse_results.rsquared_adj:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9c155-83a8-4a5c-a341-7433b354734a",
   "metadata": {},
   "source": [
    "## 4. Switching the Library\n",
    "We have used `statsmodels` up to this point. However, the go-to library when it comes to machine learning is a different library called [scikit-learn](https://scikit-learn.org/stable/), typically abbreviated as `sklearn`.\n",
    "\n",
    "Import that library. Then, using once more the data set with all features included, create another linear regression model using the class `linear_model.LinearRegression()`. Compare the coefficients between this model and the one we estimated above using `statsmodels.api.OLS`. They should be pretty much identical. Please verify that they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5f53e-7be4-47ce-b9f0-719f3f17882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to estimate linear regression using sklearn.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_sk = LinearRegression(fit_intercept=True)\n",
    "lr_sk.fit(X=X, y=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a1e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to compare regression coefficients\n",
    "# -----------------------------------------\n",
    "\n",
    "# Extract coefficients from statsmodels\n",
    "sm_params = lr_sm.params.values\n",
    "sm_params_names = lr_sm.params.index\n",
    "\n",
    "# Extract coefficients from sklearn\n",
    "# Sklearn stores intercept separately, so we concatenate it with coef_\n",
    "sklearn_params = [lr_sk.intercept_] + list(lr_sk.coef_)\n",
    "sklearn_params_names = ['Intercept'] + list(lr_sm.params.index[1:])\n",
    "\n",
    "# Combine into a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Statsmodels Coefficients': sm_params,\n",
    "    'Sklearn Coefficients': sklearn_params\n",
    "}, index=sm_params_names)\n",
    "\n",
    "# Display the DataFrame to compare coefficients side-by-side\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ba9cc-b879-4094-923f-22e1f8388af8",
   "metadata": {},
   "source": [
    "## 5. Prediction \n",
    "We next use our regression model for prediction. Feeding it with data on feature values, the estimated regression coefficients facilitate forecasting real-estate prices. \n",
    "\n",
    "For start, compute forecasts for the training data, that is, the data from which you estimated the regression coefficients. Store your forecasts in a variable, `y_hat`, and assess them in terms of mean squared error (MSE), defined as: <br><br>\n",
    "$ MSE = \\frac{1}{n}\\sum_{i=1}^{n} \\left( Y_i - \\hat{Y}_i \\right)^2 $,\n",
    "<br><br>\n",
    "with:\n",
    "- $n$ = number of data points\n",
    "- $Y$ = true values of the target variable\n",
    "- $\\hat{Y}$ = forecasts of the regression model\n",
    "\n",
    "> Hint: if unsure how to implement the MSE yourself, you can be sure that ready-made functions are available to do it for you ;)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc3e57-bbe7-4cd9-be90-12c014622ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to assess regression model forecasts using MSE\n",
    "y_hat = lr_sk.predict(X)\n",
    "mse = np.sum((y-y_hat)**2)/len(y)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abdb963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative solution using sklearn function\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_true=y, y_pred=y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b0983-acdc-4231-908f-9c1ec2818b7e",
   "metadata": {},
   "source": [
    "## Closing questions\n",
    "- We assessed the model in terms of MSE. Why is the root of the MSE often preferred in practice?\n",
    "- Beyond (R)MSE, considering all results observed in the notebook, what is your overall opinion of the regression model?\n",
    "- No model is perfect. What could we try to improve the value of the regression model for forecasting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d260551-e9c2-4fca-8c80-e987e3eaf032",
   "metadata": {},
   "source": [
    "**Space to take notes on the above points**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240d0a8",
   "metadata": {},
   "source": [
    "RMSE is preferred because it has the same scale as the target. For example, if the target is measured in EUR, than RMSE gives the EUR-amount by which the forecast is, on average, away from the target. This quantity is easier to interpret than the squared average difference.\n",
    "\n",
    "Overall, the regression model is doing a fair job. This can be deduced from the $R^2$ statistic of roughly 60%. From the EDA, we can tell that the distribution of the target is non-normal, which is a red flag for linear regression. The reason is that, for reasons of confidentiality, the target variable is clipped at a value of 5. \n",
    "\n",
    "Another issue concerns the features, which exhibit several outliers. For example, the scatter plots of predictive features versus the target (Exercise 2) indicate the adverse effect of outliers. \n",
    "\n",
    "Given we cannot do anything about the odd measurement of the target, removing or truncating outliers among the features is a good first step to improve the regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c62e5dc",
   "metadata": {},
   "source": [
    "# EXTRA Exercise (Challenging!):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8923a",
   "metadata": {},
   "source": [
    "## Backward Elimination \n",
    "We examined the effect of discarding features in a previous exercise. This task takes a more comprehensive approach toward feature selection.\n",
    "\n",
    "Relying on the following pseudo code, implement a *backward elimination* procedure, in which you repetitively discard the least important feature from the regression model.\n",
    "\n",
    "```\n",
    "    Use all features as the current set of features  \n",
    "    Do \n",
    "        Estimate a regression model using current set of features\n",
    "        Store model performance in terms of a suitable statistic\n",
    "        Identify the least important feature\n",
    "        Discard that feature from current set of features\n",
    "    Repeat until all features but one have been deleted\n",
    "   \n",
    "```\n",
    "Depict your results graphically by plotting the number of features in the regression model against model performance using the same statistic as in your backward elimination algorithm.\n",
    ">Hint: we recommend you use the function `OLS` from the  `statsmodels` library for this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34987c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your backward elimination algorithm:\n",
    "#-------------------------------------------------------\n",
    "# Initialize the current set of features\n",
    "current_features = list(X.columns)\n",
    "model_performance = []\n",
    "\n",
    "# Begin backward elimination\n",
    "while len(current_features) > 1:\n",
    "    # Add a constant to the model (intercept) and fit with statsmodels for p-values\n",
    "    X_current = sm.add_constant(X[current_features])\n",
    "    model = sm.OLS(y, X_current).fit()\n",
    "    \n",
    "    # Store model performance in terms of R-squared and current features\n",
    "    r2 = model.rsquared\n",
    "    model_performance.append((current_features.copy(), r2, model.summary()))\n",
    "\n",
    "    # Find the feature with the highest p-value (least statistically significant)\n",
    "    p_values = model.pvalues.iloc[1:]  # Exclude the intercept's p-value\n",
    "    least_significant_feature = p_values.idxmax()  # Get feature with highest p-value\n",
    "\n",
    "    # Remove the least significant feature from the current set\n",
    "    current_features.remove(least_significant_feature)\n",
    "\n",
    "# Convert performance results to DataFrame for easier inspection\n",
    "performance_df = pd.DataFrame([(features, r2) for features, r2, _ in model_performance], \n",
    "                              columns=['Features', 'R-squared'])\n",
    "print(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4433433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for visualizing the results of backward elimination\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# Extract the number of features and corresponding R-squared values for the plot\n",
    "num_features = [len(features) for features, r2, _ in model_performance]\n",
    "r2_values = [r2 for features, r2, _ in model_performance]\n",
    "\n",
    "# Plot R-squared values against the number of features\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(num_features, r2_values, marker='o')\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"R-squared\")\n",
    "plt.title(\"R-squared vs. Number of Features in Backward Elimination\")\n",
    "plt.gca().invert_xaxis()  # Invert x-axis to show decreasing number of features\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

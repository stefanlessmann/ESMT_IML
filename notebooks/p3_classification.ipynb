{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2e2f2f-70e7-4662-9aa1-07b5bb50585d",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "# Practical 3: Probability of default (PD) prediction and credit rating analysis\n",
    "<hr>\n",
    "In this practice session, we consider the logistic regression model and study how it allows us to approach classification problems. \n",
    "\n",
    "The notebook comprises demo codes, which you can execute, and small programming tasks. For some tasks, you might know the answer already. In that case, just code your solution and move on. More likely, however, you might come across a task for which you do not know immediately what is the solution and/or how to code it. This is the normal setting. In programming, we spent a lot of time on web searching for demos, tutorials or searching question-forums like *stackoverflow* to find a solution to a focal programming problem. Hence, whenever you do not know how to perform a task, try to find the answer on the internet. Web search is your friend!\n",
    "Nowadays, as seen in our first practical, you can also get help from GenAI. Feel free to use GenAI for help when needed. However, whenever you use GenAI, make sure to carefully read the explanations that come with generated code. This allows you to learn and progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c084741-bb8f-48c2-afbe-5236af75860b",
   "metadata": {
    "executionInfo": {
     "elapsed": 2466,
     "status": "ok",
     "timestamp": 1695536714957,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "wIgF2_GabxOZ"
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd477e7a-c3c4-4952-b50b-4cd3d866906c",
   "metadata": {},
   "source": [
    "## Binary classification for PD modeling\n",
    "The lecture introduced you to credit scoring, the problem of estimating the probability of credit applicants to repay debt.\n",
    "\n",
    "For this demo, we consider the *Home Equity (HMEQ)* data set from the textbook [Credit Risk Analytics](http://www.creditriskanalytics.net). It comprises information about a set of borrowers, which are categorized along demographic features and features concerning their business relationship with the lender. A binary target variable called *BAD* is provided and indicates whether a borrower has repaid their debt. \n",
    "\n",
    "\n",
    "**Info:** our GitHub repo provides a much more comprehensive analysis of this data set in our demo of a [fully-fledged machine learning pipeline](https://github.com/stefanlessmann/ESMT_IML/blob/main/notebooks/demo_ml_pipeline.ipynb). You are welcome to take a look if interested.\n",
    " \n",
    "\n",
    "### Loading and preparing the data\n",
    "Using the `Pandas` library, we can retrieve the data right from the web; specifically the GitHub repository of this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd776c45-c9d5-449e-91ba-f407dbeaaca9",
   "metadata": {
    "id": "28M_8V9LkG6Y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_location = 'https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/master/data/hmeq.csv'\n",
    "df = pd.read_csv(data_location)  # standard pandas function to load tabular data in CSV format\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f150d-6ecb-4005-8a87-39a84c71fcfd",
   "metadata": {},
   "source": [
    "Here is an overview of the data:\n",
    "- BAD: the target variable, 1=default; 0=non-default\n",
    "- LOAN: amount of the loan request\n",
    "- MORTDUE: amount due on an existing mortgage\n",
    "- VALUE: value of current property\n",
    "- REASON: DebtCon=debt consolidation; HomeImp=home improvement\n",
    "- JOB: occupational categories\n",
    "- YOJ: years at present job\n",
    "- DEROG: number of major derogatory reports\n",
    "- DELINQ: number of delinquent credit lines\n",
    "- CLAGE: age of oldest credit line in months\n",
    "- NINQ: number of recent credit inquiries\n",
    "- CLNO: number of credit lines\n",
    "- DEBTINC: debt-to-income ratio\n",
    "\n",
    "As you can see, the features aim to describe the financial situation of a borrower, which should probably tell us something about the risk of a borrower to default.\n",
    "\n",
    "Run the below code to obtain a snapshot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504665e7-8aeb-4ba1-a56e-28ba700880db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde84bc6-bf07-42dc-8623-47791e846843",
   "metadata": {},
   "source": [
    "The data requires at least a little bit of preparation to be ready for machine learning. First, we need to address the missing values. Second, two of the features, REASON and JOB, are non-numeric. Such categorical features cannot be used in a logistic regression model. We must convert them to numbers before using them. The following code addresses both problems in a quick (but also simplistic) way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85fb7f37-fbcd-41a6-9814-23b9dce5c08d",
   "metadata": {
    "id": "28M_8V9LkG6Y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert a category with k different values into k-1 binary variables. \n",
    "X = pd.get_dummies(df, dummy_na=True, drop_first=True)\n",
    "X = X.dropna().reset_index(drop=True)  # drop all cases with one or more missing value\n",
    "\n",
    "# Separate the data into a matrix of feature values and a target variable\n",
    "y = X.pop('BAD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2ea9f-4363-42a7-a5a3-ec2d037cff01",
   "metadata": {},
   "source": [
    "### Excercise 1: Plotting data for classification\n",
    "You will remember the many plots we came across when discussing regression. We also saw some analog plots for classification problems in the lecture. One of them was a 2d scatter plot displaying the bi-variate relationship between selected features and the binary target variable. \n",
    "\n",
    "![Classification problem in 2D](https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/main/resources/2d_classification_problem.png)\n",
    "\n",
    "Your first task is to create a similar plot for the credit data. In principle, you can select any combination of features that you like.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056e886-fb90-4846-a961-fe6543a6ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "x1 = ''  # select first feature of your choice\n",
    "x2 = ''  # select second feature of your choice\n",
    "\n",
    "# Write code to create the scatter plot of x1 vs. x2. \n",
    "# Make sure your plot shows the data points from different classes\n",
    "#  (good and bad payers) in different colors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9cb93-5516-40b2-8a46-59080b211679",
   "metadata": {},
   "source": [
    "For the sake of completeness, the lecture also introduced a different 2D plot, in which we chart the target variable against one of the features. Provided you have made a choice above, by assigning variable `x1` some valid feature name, you will be able to execute the below code to create this type of plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0e93e-5bf2-4b7d-a2e1-28f8811056e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to depict how the values of one feature, x1, distribute across the values of the target variable\n",
    "\n",
    "# Just to make sure you did indeed a assign x1 a valid value ;)\n",
    "assert x1 != '', \"please assign a valid feature name to varible x2, like, e.g., x2='DEBTINC'\"\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(X[x1], y, c=y)\n",
    "plt.xlabel(x1)\n",
    "plt.ylabel('BAD=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742e463-9ab9-49ca-a84e-3c1bfb004a90",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Time to estimate our first model. We will use logistic regression. Think of it as an extension of linear regression for cases in which we work with a binary target variable. The lecture will soon provide more details.\n",
    "\n",
    "Just as in linear regression, logistic regression involves model training on labelled data. The below code uses the `sklearn` library to train a logistic regression-based classification model. \n",
    "\n",
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9bfc1-ab50-465e-934f-1034f587ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(random_state=888).fit(X, y)  # we define a random_state to ensure that we get the same results when re-running this cell multiple times\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb9920-d1b3-4a19-9dfa-0a14bb7210bc",
   "metadata": {},
   "source": [
    "Note that the `sklearn` implementation does not provide an informative summary, as did the library `statsmodels` in our last practical. In brief, this is because `sklearn` is designed to support prediction. Let's demonstrate how to do this, that is compute predictions using the trained model. For simplicity, we compute prediction for the training data. You already learnt that this is inappropriate. We do it here to keep things simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2856dfd-fe11-4ed1-803e-63f66c58b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X)  # simple way to compute predictions using logistic regression and any other machine learning model in sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4565bc58-709f-4b00-919c-82ea24b30082",
   "metadata": {},
   "source": [
    "Likely, you are also interested to assess the model. There is an easy way to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7039a29-fedc-4932-a098-8052bf57657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = model.score(X, y)  # Call a general purpose evaluation function and obtain a (quality ) score of the model\n",
    "print('Logit model achieves a score of {:.3f} %'.format(perf*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7202a-455b-44a3-9d16-78c40a5ea1a3",
   "metadata": {},
   "source": [
    "### Exercise 2: Diagnosing predictions\n",
    "A score of above 90 percent sounds very good. Actually, it is not, and your task is to find out why. Let's break it down into pieces.\n",
    "\n",
    "#### A) What score?\n",
    "Check the sklearn documentation to understand what kind of score the function `score()` has provided. What is it that we see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03225801-d197-471b-a2be-c88aff8d126e",
   "metadata": {},
   "source": [
    "**Your answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a24e3-05d9-4439-8958-f7459cd0af4f",
   "metadata": {},
   "source": [
    "#### B) Is it good or is it bad?\n",
    "Interpreting our score will be easier if we compare it to a baseline. But what baseline? We face a classification problem. There are two classes, good payers and bad payers, and we aim to tell these apart. Come up with a very basic strategy to solve the classification problem without using any model. Write a piece of code to calculate the performance of your super-basic strategy. \n",
    "> Hint: if you feel a bit lost, consider web searching for *dummy classifier* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca09e4-5475-48f5-b938-10d8d6cd79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to calculate as baseline the score of a very basic classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c128e4-0a74-47fc-b9d7-c54f894caec5",
   "metadata": {},
   "source": [
    "If you succeeded with the previous task, you will have found that a super-basic - stupid - classifier does as well as logistic regression. This is a devastating result. Our logistic regression classifier is just as good as a naive classifier, which always predicts the majority class. Put differently, the logistic regression *appears* completely useless.\n",
    "\n",
    "Note that our approach to compute the score of the naive classifier assumes that the positive class with $Y=1$ is the minority class. While this is typically the case, we should acknowledge that our approach is simplistic. It would be better to first establish which of the two classes is the majority class and to then use the fraction of that class as the accuracy score of a dummy classifier. While not too difficult, we leave this extension for the interested to perform and move on with probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594ac70-c675-4eae-951a-2210ee923dc8",
   "metadata": {},
   "source": [
    "#### C) What about probabilities?\n",
    "Exactly, what about probabilities? The lecture introduced classification as a machine learning setup aimed at predicting class membership probabilities. So logistic regression should answer questions such as \"what is the estimated probability of the first credit applicant in our data set to repay?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394a7cb-f1d2-4981-b0e0-7c73eb457a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to print the prediction of logistic regression for the first data point \n",
    "# in our matrix X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8bc848-44a1-4a67-915f-93f335badc9d",
   "metadata": {},
   "source": [
    "Solving the above task, you will find that the prediction does not look like a probability. Examine this point in more detail. To that end, write code that tells you what *distinct* values logistic regression predicts. Put differently, your code should print out all unique values that logistic regression has predicted across all data points. \n",
    "\n",
    "Briefly explain your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a644d-a304-41c2-b07c-3c5177e1ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to find out the distinct values of the predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b615d4-5931-42d3-9d64-57db12b4d03e",
   "metadata": {},
   "source": [
    "**How to interpret the distinct predictions? Briefly explain.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a016d75f-8cda-47e8-b071-8efca464c6a0",
   "metadata": {},
   "source": [
    "Finally, we come back to the innocent question asked before, \"what is the estimated probability of the first credit applicant in our data set to repay?\". Given our previous analysis has not answered this question it is about time to. Write code to find out the estimated probability of the first applicant to be a bad credit risk?\n",
    "\n",
    "Just in case, mathematically, we could represent the sought probability as $\\hat{p}(BAD==1|X_1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a4de6-29db-4e86-9356-cac4fe339120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to obtain probability predictions from the logit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173f614-b287-4e1b-b4a3-2937c88e1383",
   "metadata": {},
   "source": [
    "### Visualizing the logistic regression\n",
    "\n",
    "#### Exercise 3: One more logistic regression\n",
    "Please estimate a second logistic regression model. This time, use only two features. Exercise 1 has asked you to examine combinations of features. Just continue with the two features you selected there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28c593-d90a-43c4-92f6-a2f20364ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our first feature is:\\t', x1)\n",
    "print('Our second feature is:\\t', x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "127568a4-102e-4b5a-9b2b-9a7d522421fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to estimate a logistic regression classifier using only the two above features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c727e2-8442-4b32-b857-a747cc36ba62",
   "metadata": {},
   "source": [
    "#### The visual logistic regression\n",
    "As you will have guessed, the point of the above exercise 4 was only to obtain a logistic regression model that we can plot; hence the need to select two features. \n",
    "The visualization is somewhat complex. Thus, all code is readily available for you. Below we provide a function `plot_logit_decision_surface()`. \n",
    "**Do not be put off by the length of the code.** You are not supposed to look through the function at this point. Of course you can, but do not allow it to confuse you. It is a function to create a plot. That is all you need to know for now. Please execute the cell to make sure you can use the function in the next exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a9a3c5d-614f-4ffb-9fd2-f67d66b0ebc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_logit_decision_surface(model, data, x1, x2, save_fig=False):\n",
    "    '''\n",
    "        Visualization of logistic regression in 2D\n",
    "        \n",
    "        Creates a plot depicting the distribution of the input\n",
    "        data along two dimensions and the probability predictions\n",
    "        of a logistic regression model. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model :   An instance of the sklearn class LogisticRegression,  which        \n",
    "                  has been trained on the input data.\n",
    "\n",
    "        data  :   Pandas data frame providing the feature values.\n",
    "\n",
    "        x1, x2:   The function plots the results of logistic regression in\n",
    "                  two dimensions. The parameters x1 and x2 give the names\n",
    "                  of the features used for plotting. These features will be\n",
    "                  extracted from the data frame.\n",
    "\n",
    "        save_fig: Binary variable allowing you to save the figure as a PNG image. \n",
    "                  Default: False\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        The function does not return a result. It's purpose is to visualize \n",
    "        logistic regression model. The corresponding plot is the only output.\n",
    "    '''\n",
    "\n",
    "    #if len(model.coef_.ravel())!=2:\n",
    "    #    raise Exception('Please estimate a logit model using only two features!')\n",
    "    # Define some variables to govern the plot\n",
    "    bounds = data.describe().loc[[\"min\", \"max\"]][[x1, x2]].to_numpy()  # value ranges of the two features\n",
    "    eps = 5  # tolerance parameter \n",
    "\n",
    "    # Create hypothetical data points spanning the entire range of feature values.\n",
    "    # We need these to get from our logistic regression model a probability prediction\n",
    "    # for every possible data point\n",
    "    xx, yy = np.mgrid[(bounds[0,0]-eps):(bounds[1,0]+eps), (bounds[0,1]-eps):(bounds[1,1]+eps)]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    # Perhaps the logistic regression model was fitted using the full data frame. \n",
    "    # To also work in that case, we extract the estimated regression coefficients \n",
    "    # corresponding to the two features we consider for plotting\n",
    "    feature_to_index = {name: idx for idx, name in enumerate(model.feature_names_in_)}  # create a dic as intermediate step\n",
    "    indices = [feature_to_index[f] for f in [x1, x2]]  # Find the indices of our two features of interest using the dic\n",
    "    w = model.coef_.ravel()[indices]  # estimated regression coefficients\n",
    "    b = model.intercept_  # estimated intercept of the logistic regression model\n",
    "\n",
    "    # Compute probability predictions over the entire space of possible feature values\n",
    "    # In the interest of robustness, we manually compute the logistic regression predictions\n",
    "    # using the regression coefficients extracted above\n",
    "    probs = 1/(1+np.exp(-(np.dot(grid, w.reshape(2,-1))+b))).reshape(xx.shape)\n",
    "\n",
    "    # We are finally ready to create our visualization\n",
    "    f, ax = plt.subplots(figsize=(8, 6))  # new figure\n",
    "    # Contour plot of the probability predictions across the entire feature range\n",
    "    contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\", vmin=0, vmax=1)  \n",
    "    ax_c = f.colorbar(contour)\n",
    "    ax_c.set_label(\"$\\hat{p}(y=1|X)$\")\n",
    "    ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "    # Scatter plot of the actual data\n",
    "    ax.scatter(data[x1], data[x2], c=y, s=50, cmap=\"RdBu\", vmin=0, vmax=1,\n",
    "               edgecolor=\"white\", linewidth=1);\n",
    "    plt.xlabel(x1)\n",
    "    plt.ylabel(x2)\n",
    "    if save_fig==True:\n",
    "        plt.savefig('logit_contour.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0962265-96b0-45d9-8885-34b5ea6e5a95",
   "metadata": {},
   "source": [
    "#### Exercise 4: Surface plot\n",
    "We are almost ready. Also run the next cell, which will give you some instructions on how to use the plotting function. Note that this code also works for other functions. Just put a '?' in front of a function call and run the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797af5bd-df50-473f-8ece-bb6136ce3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "?plot_logit_decision_surface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca16bb2-521e-4a19-b4ea-51aa70d0e180",
   "metadata": {},
   "source": [
    "I guess your next task is obvious. Write code to call the function providing the necessary parameters so that it can do its job. If used correctly, the function will create a plot like this one:\n",
    "![Contour plot of logistic regression model](https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/main/resources/logit_contour.png)\n",
    "\n",
    "Let's if it works for you..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de265c-c77f-47c4-a90d-a5896197f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to call the function plot_logit_decision_surface()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c211ba51-662f-4ced-b428-4dceb0e9d51c",
   "metadata": {},
   "source": [
    "Finally, and hoping you got a nice contour plot of your logistic regression model, it is time to pause and think about the plot. It tells you a lot about how logistic regression works and the output you obtain. This is something we need to discuss in class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc535c0-2689-48f6-9670-d73c08fb67bd",
   "metadata": {},
   "source": [
    "# Well done! This was another comprehensive set of exercises."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B0rxPs4QEGtz",
    "27sCENzmoGcX"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

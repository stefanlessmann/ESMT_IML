{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2e2f2f-70e7-4662-9aa1-07b5bb50585d",
   "metadata": {
    "id": "ce46c39d-cfa8-4bca-82b2-c6364fd44819"
   },
   "source": [
    "# Practical 3: Classification for probability to default (PD) modeling\n",
    "<hr>\n",
    "In this practice session, we consider the logistic regression model and study how it allows us to approach classification problems. \n",
    "\n",
    "The notebook comprises demo codes, which you can execute, and small programming tasks. For some tasks, you might know the answer already. In that case, just code your solution and move on. More likely, however, you might come across a task for which you do not know immediately what is the solution and/or how to code it. This is the normal. In programming, we spent a lot of time on web searching for demos, tutorials or searching question-forums like *stackoverflow* to find a solution to a programming problem. Hence, whenever you do not know *how* to perform a task, search for the answer on the internet. Web search is your friend!\n",
    "Nowadays, as seen in our first practical, you can also get help from GenAI. Feel free to use GenAI for help when needed. However, when using GenAI, make sure to carefully read the explanations that come with generated code. This allows you to learn and progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c084741-bb8f-48c2-afbe-5236af75860b",
   "metadata": {
    "executionInfo": {
     "elapsed": 2466,
     "status": "ok",
     "timestamp": 1695536714957,
     "user": {
      "displayName": "Ben Joshua Fliegener",
      "userId": "06969016385245233563"
     },
     "user_tz": -120
    },
    "id": "wIgF2_GabxOZ"
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd477e7a-c3c4-4952-b50b-4cd3d866906c",
   "metadata": {},
   "source": [
    "# Binary classification for PD modeling\n",
    "The lecture introduced you to credit scoring, the problem of estimating the probability of credit applicants to repay debt.\n",
    "<p align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/master/credit_scoring.png\" alt=\"Credit Scoring\" width=\"640\" />\n",
    "</p>\n",
    "\n",
    "For this demo, we consider the *Home Equity (HMEQ)* data set from the textbook [Credit Risk Analytics](http://www.creditriskanalytics.net). It comprises information about a set of borrowers, which are categorized along demographic features and features concerning their business relationship with the lender. A binary target variable called *BAD* is provided and indicates whether a borrower has repaid their debt. \n",
    "\n",
    "\n",
    "**Info:** our GitHub repo provides a much more comprehensive analysis of this data set in our demo of a [fully-fledged machine learning pipeline](https://github.com/stefanlessmann/ESMT_IML/blob/main/notebooks/demo_ml_pipeline.ipynb). You are welcome to take a look if interested.\n",
    " \n",
    "\n",
    "## Loading and preparing the data\n",
    "Using the `Pandas` library, we can retrieve the data right from the web; specifically the GitHub repository of this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd776c45-c9d5-449e-91ba-f407dbeaaca9",
   "metadata": {
    "id": "28M_8V9LkG6Y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_location = 'https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/master/data/hmeq.csv'\n",
    "df = pd.read_csv(data_location)  # standard pandas function to load tabular data in CSV format\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f150d-6ecb-4005-8a87-39a84c71fcfd",
   "metadata": {},
   "source": [
    "Here is an overview of the data:\n",
    "- BAD: the target variable, 1=default; 0=non-default\n",
    "- LOAN: amount of the loan request\n",
    "- MORTDUE: amount due on an existing mortgage\n",
    "- VALUE: value of current property\n",
    "- REASON: DebtCon=debt consolidation; HomeImp=home improvement\n",
    "- JOB: occupational categories\n",
    "- YOJ: years at present job\n",
    "- DEROG: number of major derogatory reports\n",
    "- DELINQ: number of delinquent credit lines\n",
    "- CLAGE: age of oldest credit line in months\n",
    "- NINQ: number of recent credit inquiries\n",
    "- CLNO: number of credit lines\n",
    "- DEBTINC: debt-to-income ratio\n",
    "\n",
    "As you can see, the features aim to describe the financial situation of a borrower, which should probably tell us something about the risk of a borrower to default.\n",
    "\n",
    "Run the below code to obtain a snapshot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504665e7-8aeb-4ba1-a56e-28ba700880db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde84bc6-bf07-42dc-8623-47791e846843",
   "metadata": {},
   "source": [
    "The data requires at least a little bit of preparation to be ready for machine learning. First, we need to address the missing values. Second, two of the features, REASON and JOB, are non-numeric. Such categorical features cannot be used in a logistic regression model. We must convert them to numbers before using them. The following code addresses both problems in a quick (but also simplistic) way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb7f37-fbcd-41a6-9814-23b9dce5c08d",
   "metadata": {
    "id": "28M_8V9LkG6Y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert a category with k different values into k-1 binary variables. \n",
    "X = pd.get_dummies(df, dummy_na=True, drop_first=True)\n",
    "X = X.dropna().reset_index(drop=True)  # drop all cases with one or more missing value\n",
    "X[\"LOAN\"] = X[\"LOAN\"].astype(float)  # ensure that LOAN is stored as a float like all other numerical features\n",
    "\n",
    "\n",
    "# Separate the data into a matrix of feature values and a target variable\n",
    "y = X.pop('BAD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2ea9f-4363-42a7-a5a3-ec2d037cff01",
   "metadata": {},
   "source": [
    "## Excercise 1: Plotting data for classification\n",
    "### a) Regression-like scatter plot\n",
    "You will remember the many plots we came across when discussing linear regression. Let us try to create a similar plot for our classification problem. Specifically, create a **scatter plot** of the target variable `y` and the feature `LOAN`. Once a first version of the plot, adjust it such that cases (i.e., data points) from different classes (i.e., `BAD` = 0 and `BAD` = 1) are displayed in different colors.\n",
    "\n",
    ">Extension (optional): we picked the feature `LOAN` for this plot. Using the function `.subplots()` from *Matplotlib*, you could create a whole grid of scatter plots, one for each feature. If trying to solve the extension, I suggest you restrict the analysis to numerical features using the *Pandas* function `.select_dtypes(include=['number'])`    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e92559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1a: scatter plot of target versus one feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb535708",
   "metadata": {},
   "source": [
    "A striking result of the scatter plot stems form the binary target variable `y`. Along this axis, all data points are either zero or one. The key question is whether, based on the value of the feature, e.g., LOAN, we can infer whether the target is more likely to be zero or one. What is your opinion? \n",
    "\n",
    "### b) Scatter plot of two features \n",
    "Given that the *standard* scatter plot is somewhat dominated by the binary target, it makes sense to explore alternative visualizations. For instance, we could consider an augmented scatter plot of two features, for example, `LOAN` and `VALUE`, and color the data points according to the target variable `BAD`. This would allow us to see whether defaulters and non-defaulters cluster in different regions of the feature space spanned by `LOAN` and `VALUE`. Try to also create this visualization. In principle, you can select any two features that you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056e886-fb90-4846-a961-fe6543a6ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1b: scatter plot of two features\n",
    "X1 = 'LOAN'  # select first feature of your choice\n",
    "X2 = 'VALUE'  # select second feature of your choice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9cb93-5516-40b2-8a46-59080b211679",
   "metadata": {},
   "source": [
    "Assuming you solved exercise 1b, you will agree that the scatterplot of LOAN vs. VALUE does not seem very informative. More specifically, we cannot observe evidence that, using these two features, a predictive model would be able to distinguish good and bad payers well. Examining this question, which feature or which features are predictive of the target was also the subject of Exercise 1a. For the sake of completeness, we illustrate some more common visualizations we could consider to examine individual features predictiveness in a binary classification context. \n",
    "\n",
    "### Alternative visualizations for binary classification problems\n",
    "#### Examining one numerical feature \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0e93e-5bf2-4b7d-a2e1-28f8811056e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of one numerical feature grouped by the binary target\n",
    "feature = 'LOAN'  # select feature of your choice\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x=y, y=X[feature])\n",
    "plt.title(f'Boxplot of {feature} grouped by BAD')\n",
    "plt.xlabel('BAD (0 = Non-Default, 1 = Default)')\n",
    "plt.ylabel('LOAN Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b880a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of one numerical feature grouped by the binary target\n",
    "feature = 'LOAN'  # select feature of your choice\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(x=X[feature], hue=y )\n",
    "plt.title(f'Histogram of {feature} grouped by BAD')\n",
    "plt.xlabel('BAD (0 = Non-Default, 1 = Default)')\n",
    "plt.ylabel('LOAN Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a378e",
   "metadata": {},
   "source": [
    "#### Examining one categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a29990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked bar plot of one categorical feature grouped by the binary target\n",
    "categorical_feature = 'JOB'  # select categorical feature of your choice\n",
    "contingency_table = pd.crosstab(df[categorical_feature], y)\n",
    "contingency_table.plot(kind='bar', stacked=True,  figsize=(6, 4))\n",
    "plt.title(f'Stacked Bar Plot of {categorical_feature} grouped by BAD')\n",
    "plt.xlabel(categorical_feature)\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='BAD', labels=['Non-Default (0)', 'Default (1)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dadc06d",
   "metadata": {},
   "source": [
    "#### Examining one numerical together with a categorical feature and their impact on the target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot of one numerical feature grouped by the binary target\n",
    "numerical_feature = 'LOAN'  # select numerical feature of your choice\n",
    "categorical_featuer = 'REASON'  # select categorical feature of your choice\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.violinplot(x=df[categorical_feature], y=X[numerical_feature], hue=y, split=True,  inner=\"quart\", alpha=0.6)\n",
    "plt.title(f'Violin Plot of {numerical_feature} stratefied by {categorical_feature} grouped by BAD') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742e463-9ab9-49ca-a84e-3c1bfb004a90",
   "metadata": {},
   "source": [
    "# Classification models\n",
    "The lecture introduced you to the logistic function and the logistic regression model. This part revisits both concepts. \n",
    "\n",
    "Recall the formula of the logistic function:\n",
    "\n",
    "$$ \\frac{1}{1+exp(-\\eta)} $$\n",
    "where\n",
    "$$ \\eta = b + w_1 x_1 + w_2 x_2 + ... + w_p x_p $$\n",
    "\n",
    "Also recall how we can use the logistic function to *fit* observed data in a binary classification problem. \n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/master/logistic_function_playing.png\" alt=\"Logistic function\" width=\"640\" />\n",
    "</p>\n",
    "\n",
    "The following code cell includes some prepared code that generates synthetic data of a binary classification problem with a single feature. The data is displayed in a scatter plot, just as in Exercise 1. The plot also includes the logistic function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c72140",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 2: Fitting a logistic regression model by hand\n",
    "The code allows you to set the two parameters of the logistic function, the intercept $b$ and the coefficient $w$ that scales the values of the feature $x$. This is shown in the above equation, whereby, in our simplified setting, we have only a single feature $x_1$. Your task is to find suitable values for these two parameters. Execute with different choice and inspect how well your logistic function models the classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select parameters for the logistic function 1 + exp(constant + multiplier * x)\n",
    "b = 0  # intercept analog\n",
    "w = 1  # coefficient analog\n",
    "\n",
    "# Creating synthetic data for a toy classification problem with one (informative feature) and a binary target\n",
    "def toy_data(n_samples=200):\n",
    "    '''\n",
    "    Generate synthetic data for binary classification with one predictive numerical feature.\n",
    "    Feature values of x are generated randomly from a uniform distribution in the range [-5, 5].\n",
    "    The binary target y is generated based on x, such that higher values of x correspond to a higher probability of y=1.\n",
    "    To achieve this, we sample y from a Bernoulli distribution where the success probability is determined by a logistic function of x.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------- \n",
    "    n_samples : int\n",
    "        Number of samples to generate  (default = 100)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    x : array-like, shape (n_samples,)\n",
    "        Generated feature values    \n",
    "    y : array-like, shape (n_samples,)\n",
    "        Generated binary target values (0 or 1)\n",
    "    Example:\n",
    "    --------    \n",
    "    x, y = toy_data(n_samples=150)\n",
    "    '''\n",
    "    np.random.seed(123)  # ensure reproducibility by fixing the random seed\n",
    "    x = np.random.uniform(-5, 5, n_samples)  # Generate feature x in the range [-5, 5]\n",
    "\n",
    "    # Set the true parameters for the logistic function\n",
    "    true_intercept = 0.13  # intercept\n",
    "    true_coef = -1.88  # coefficient for x\n",
    "\n",
    "    # Generate binary target y based on x (x is predictive of y)\n",
    "    prob_y = 1 / (1 + np.exp(true_intercept + true_coef * x))  # Higher x values -> higher probability of y=1\n",
    "    y = np.random.binomial(1, prob_y)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Logistic function with user parameters\n",
    "def logistic_function(x, multiplier=w, constant=b):\n",
    "    \"\"\"\n",
    "    Computes logistic function: 1 / (1 + exp(constant+ multiplier * x)))\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        Input values\n",
    "    multiplier : float, default=1\n",
    "        Scaling factor for x\n",
    "    constant : float, default=0\n",
    "        Constant added to the product of x and multiplier (analog to the intercept in linear regression)\n",
    "    \n",
    "    Returns:\n",
    "    --------    \n",
    "    array-like\n",
    "        Computed logistic function values\n",
    "    Example:\n",
    "    --------\n",
    "    y = logistic_function(x, multiplier=2, constant=-1)\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(constant + multiplier * x ))\n",
    "\n",
    "# Generate synthetic data\n",
    "x, y_synthetic = toy_data(n_samples=200)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot of data\n",
    "sns.scatterplot(x=x, y=y_synthetic, hue=y_synthetic, alpha=0.6)\n",
    "\n",
    "# Plot logistic function\n",
    "x_range = np.linspace(-5, 5, 300)\n",
    "# User parameters: sign=1, constant=0, multiplier=1.5 (matching data generation)\n",
    "y_logistic = logistic_function(x_range, constant=b, multiplier=w)\n",
    "plt.plot(x_range, y_logistic, c='purple', linewidth=2, label='Logistic function')\n",
    "\n",
    "plt.xlabel('Feature $x$')\n",
    "plt.ylabel('Target $y$ / Probability')\n",
    "plt.title('Binary Classification: Feature x vs Target y with Logistic Function')\n",
    "plt.legend(title='Target')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa962253",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Exercises 2 sketches the main principles of fitting a classification model, more specifically a logistic regression. In practice, model fitting involves minimizing the (binary) cross-entropy loss functions over the observed *labeled* data using gradient-based algorithms. We discuss this in more detail in the lecture. Here, we illustrate how to fit a logistic regression model in Python using the `sklearn` library.\n",
    "\n",
    ">Note that this library is designed for supervised machine learning. If your goal was to use logisitc regression for explanatory modeling, for example, to understand the effect of features on the target variable, compute elasticities, and so on, then `sklearn` would be a bad choice. It does not support explanatory modeling; certainly not as well as alternative libraries would. You could consider the  `statsmodels` for developing explanatory logistic regression models. Explanatory modeling in general and using logistic regression in particular is discussed in your *Econometrics *class. We focus on predictive modeling.\n",
    "\n",
    "### Model training\n",
    "Just as in linear regression, logistic regression involves model training on labelled data. The below code uses the `sklearn` library to train a logistic regression ,model on our HMEQ credit risk dataset. To ensure the code is self-contained, we repeat the data loading and preparation steps from above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ad1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat data loading and preparation (see above parts for details)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data_location = 'https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/master/data/hmeq.csv'\n",
    "df = pd.read_csv(data_location) \n",
    "X = df.dropna().reset_index(drop=True)  \n",
    "y = X.pop('BAD')\n",
    "numerical_features = X.select_dtypes(include=['number']).columns\n",
    "X = pd.get_dummies(X, dummy_na=True, drop_first=True)\n",
    "scaler = StandardScaler()\n",
    "X[numerical_features] = scaler.fit_transform(X[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9bfc1-ab50-465e-934f-1034f587ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression().fit(X, y)  # we define a random_state to ensure that we get the same results when re-running this cell multiple times\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb9920-d1b3-4a19-9dfa-0a14bb7210bc",
   "metadata": {},
   "source": [
    "As said, `sklearn` is designed for predictive modeling. Therefore, our logit model provides a function `.predict()` to obtain predictions for some input data. For simplicity, we compute prediction for the training data. You already learnt that this is inappropriate. We do it here to keep things simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2856dfd-fe11-4ed1-803e-63f66c58b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X)  # simple way to compute predictions using logistic regression and any other machine learning model in sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4565bc58-709f-4b00-919c-82ea24b30082",
   "metadata": {},
   "source": [
    "Likely, you are also interested to assess the model. There is an easy way to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7039a29-fedc-4932-a098-8052bf57657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = model.score(X, y)  # Call a general purpose evaluation function and obtain a (quality ) score of the model\n",
    "print('Logit model achieves a score of {:.3f} %'.format(perf*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7202a-455b-44a3-9d16-78c40a5ea1a3",
   "metadata": {},
   "source": [
    "### Exercise 3: Diagnosing predictions\n",
    "A score of above 90 percent sounds very good. Actually, it is not, and your task is to find out why. Let's break it down into pieces.\n",
    "\n",
    "#### a) What score?\n",
    "Check the [sklearn documentation](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html) to understand what kind of score the function `score()` has provided. What is it that we see?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03225801-d197-471b-a2be-c88aff8d126e",
   "metadata": {},
   "source": [
    "**Your answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a24e3-05d9-4439-8958-f7459cd0af4f",
   "metadata": {},
   "source": [
    "#### b) Is it good or is it bad?\n",
    "Interpreting our score will be easier if we compare it to a baseline. But what baseline? We face a classification problem. There are two classes, good payers and bad payers, and we aim to tell these apart. A very basic strategy could be to classify every observation into the majority class. Write code to implement this naive strategy and compute the percentage of correct classifications. You manually determine which class, 0 or 1, is the majority. However, a better approach would be to first determine the majority class by writing some code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca09e4-5475-48f5-b938-10d8d6cd79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to calculate as the percentage of correct classifications of a naive classifier that classifies every case into the majority class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c128e4-0a74-47fc-b9d7-c54f894caec5",
   "metadata": {},
   "source": [
    "If you succeeded with the previous task, you will have found that a super-basic - stupid - classifier also performs very well, which puts our previous result for the logistic regression into perspective. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594ac70-c675-4eae-951a-2210ee923dc8",
   "metadata": {},
   "source": [
    "#### c) What about probabilities?\n",
    "Exactly, what about probabilities? The lecture introduced classification as a machine learning setup aimed at predicting class membership probabilities. So logistic regression should answer questions such as \"what is the estimated probability of the first credit applicant in our data set to repay?\" To do this, we must use a different `slklearn` function, namely `.predict_proba()`. Use this function to compute the class membership probabilities for the first five credit applicants in our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394a7cb-f1d2-4981-b0e0-7c73eb457a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probabilistic predictions using .predict_proba()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173f614-b287-4e1b-b4a3-2937c88e1383",
   "metadata": {},
   "source": [
    "### Visualizing the logistic regression\n",
    "Now that we can compute probabilisitc predictions, we can conclude this session with a nice visualization of the logistic regression model fitted above. More specifically, we need to fit one more model, this time using only two features. This allows us to nicely visualize the decision boundary of the logistic regression in a two-dimensional feature space (i.e., a scatterplot).\n",
    "#### Exercise 4: One more logistic regression\n",
    "Please estimate a second logistic regression model using only two features. In principal, you could use any combination of features. Exercise 1 has shed light on which features might be more informative. Consider drawing on this knowledge to select meaningful features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127568a4-102e-4b5a-9b2b-9a7d522421fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to estimate a logistic regression classifier using only the two above features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c727e2-8442-4b32-b857-a747cc36ba62",
   "metadata": {},
   "source": [
    "#### The visual logistic regression\n",
    "As you will have guessed, the point of the above exercise was only to obtain a logistic regression model that we can plot; hence the need to select two features. \n",
    "The visualization is somewhat complex. Thus, all code is readily available for you. Below we provide a function `plot_logit_decision_surface()`. \n",
    "**Do not be put off by the length of the code.** You are not supposed to look through the function at this point. Of course you can, but do not allow it to confuse you. It is a function to create a plot. That is all you need to know for now. Please execute the cell to make sure you can use the function in the next exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a3c5d-614f-4ffb-9fd2-f67d66b0ebc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_logit_decision_surface(model, data, x1=\"LOAN\", x2=\"VALUE\"):\n",
    "    '''\n",
    "        Visualization of logistic regression in 2D\n",
    "        \n",
    "        Creates a plot depicting the distribution of the input\n",
    "        data along two dimensions and the probability predictions\n",
    "        of a logistic regression model. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model :   An instance of the sklearn class LogisticRegression,  which        \n",
    "                  has been trained on the input data.\n",
    "\n",
    "        data  :   Pandas data frame providing the feature values.\n",
    "\n",
    "        x1, x2:   The function plots the results of logistic regression in\n",
    "                  two dimensions. The parameters x1 and x2 give the names\n",
    "                  of the features used for plotting. These features will be\n",
    "                  extracted from the data frame.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        The function does not return a result. It's purpose is to visualize \n",
    "        logistic regression model. The corresponding plot is the only output.\n",
    "    '''\n",
    "\n",
    "    #if len(model.coef_.ravel())!=2:\n",
    "    #    raise Exception('Please estimate a logit model using only two features!')\n",
    "    # Define some variables to govern the plot\n",
    "    bounds = data.describe().loc[[\"min\", \"max\"]][[x1, x2]].to_numpy()  # value ranges of the two features\n",
    "    eps = 5  # tolerance parameter \n",
    "\n",
    "    # Create hypothetical data points spanning the entire range of feature values.\n",
    "    # We need these to get from our logistic regression model a probability prediction\n",
    "    # for every possible data point\n",
    "    xx, yy = np.mgrid[(bounds[0,0]-eps):(bounds[1,0]+eps), (bounds[0,1]-eps):(bounds[1,1]+eps)]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    # Perhaps the logistic regression model was fitted using the full data frame. \n",
    "    # To also work in that case, we extract the estimated regression coefficients \n",
    "    # corresponding to the two features we consider for plotting\n",
    "    feature_to_index = {name: idx for idx, name in enumerate(model.feature_names_in_)}  # create a dic as intermediate step\n",
    "    indices = [feature_to_index[f] for f in [x1, x2]]  # Find the indices of our two features of interest using the dic\n",
    "    w = model.coef_.ravel()[indices]  # estimated regression coefficients\n",
    "    b = model.intercept_  # estimated intercept of the logistic regression model\n",
    "\n",
    "    # Compute probability predictions over the entire space of possible feature values\n",
    "    # In the interest of robustness, we manually compute the logistic regression predictions\n",
    "    # using the regression coefficients extracted above\n",
    "    probs = 1/(1+np.exp(-(np.dot(grid, w.reshape(2,-1))+b))).reshape(xx.shape)\n",
    "\n",
    "    # We are finally ready to create our visualization\n",
    "    f, ax = plt.subplots(figsize=(8, 6))  # new figure\n",
    "    # Contour plot of the probability predictions across the entire feature range\n",
    "    contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\", vmin=0, vmax=1)  \n",
    "    ax_c = f.colorbar(contour)\n",
    "    ax_c.set_label(\"$\\hat{p}(y=1|X)$\")\n",
    "    ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "    # Scatter plot of the actual data\n",
    "    ax.scatter(data[x1], data[x2], c=y, s=50, cmap=\"RdBu\", vmin=0, vmax=1,\n",
    "               edgecolor=\"white\", linewidth=1);\n",
    "    plt.xlabel(x1)\n",
    "    plt.ylabel(x2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0962265-96b0-45d9-8885-34b5ea6e5a95",
   "metadata": {},
   "source": [
    "#### Exercise 5: Surface plot\n",
    "We are almost ready. To finish, write a piece of code to call our `plot_logit_decision_surface()`. Familiarize yourself with the function's interface first. Then, invoking it shohuld not be too difficult. \n",
    "I guess your next task is obvious. Write code to call the function providing the necessary parameters so that it can do its job. If used correctly, the function will create a plot like this one:\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/main/resources/logit_contour.png\" alt=\"Logit decision surface\" width=\"640\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "Let's if it works for you..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de265c-c77f-47c4-a90d-a5896197f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to call the function plot_logit_decision_surface()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c211ba51-662f-4ced-b428-4dceb0e9d51c",
   "metadata": {},
   "source": [
    "Finally, and hoping you got a nice contour plot of your logistic regression model, it is time to pause and think about the plot. It tells you a lot about how logistic regression works and the output you obtain. This is something we need to discuss in class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc535c0-2689-48f6-9670-d73c08fb67bd",
   "metadata": {},
   "source": [
    "# Well done! This was another comprehensive set of exercises."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B0rxPs4QEGtz",
    "27sCENzmoGcX"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bads310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

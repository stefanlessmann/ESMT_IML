{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84bf70f4-915d-4738-b78c-700b27b06f38",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/stefanlessmann/ESMT_IML/blob/main/notebooks/p4_model_evaluation.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32daf08-0c21-4ea8-8c23-b76b4be5b806",
   "metadata": {},
   "source": [
    "# Practical 4: Prediction Model Evaluation\n",
    "<hr>\n",
    "We have learnt about linear and logistic regression. These methods facilitate supervised learning and enable us to approach prediction problems with a numerical (e.g., price) or categorical (e.g., repayment behavior=good or bad) target variable. \n",
    "\n",
    "In this session, we focus on practices to assess the accuracy of predictive models based on linear- or logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fc25b5-773f-47f3-a832-54fd0338dac4",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "We will use the two data sets introduced in previous sessions, the *California Housing* data set from [practical #2](https://github.com/stefanlessmann/ESMT_IML/blob/main/notebooks/p2_modeling_housing_prices.ipynb) and the *HMEQ* data set from [practical #3](https://github.com/stefanlessmann/ESMT_IML/blob/main/notebooks/p3_classification.ipynb), which represent, respectively, a regression associated with modeling median house prices at the district level and a classification problem associated with predicting the repayment behavior of credit applicants (aka credit risk modeling). Below we reproduce codes from earlier sessions to import standard libraries, load the data sets, and perform some rudimentary data preprocessing where needed. Just execute those codes and we are ready to go.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b49a3-f1ff-41fa-86d2-473b80a82bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Loading the California Housing data set\n",
    "#--------------------------------------------------------\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "calh = fetch_california_housing(as_frame=True)  # get the data as a Pandas dataframe\n",
    "# separate the data into feature matrix X and target variable y\n",
    "X_calh = calh.data\n",
    "y_calh = calh.target\n",
    "print('Loaded California Housing data set with dimension (rows x columns) {} x {}'.format(*X_calh.shape))\n",
    "#--------------------------------------------------------\n",
    "# Training (aka fitting) a linear regression model\n",
    "#--------------------------------------------------------\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression().fit(X_calh, y_calh)\n",
    "#--------------------------------------------------------\n",
    "# Loading the HMEQ credit risk data set\n",
    "#--------------------------------------------------------\n",
    "data_url = 'https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/master/data/hmeq.csv'\n",
    "hmeq = pd.read_csv(data_url)  # standard pandas function to load tabular data in CSV format\n",
    "\n",
    "# Convert a category with k different values into k-1 binary variables. \n",
    "X_hmeq = pd.get_dummies(hmeq, dummy_na=True, drop_first=True)\n",
    "X_hmeq = X_hmeq.dropna().reset_index(drop=True)  # drop all cases with one or more missing value\n",
    "\n",
    "# Separate the data into a matrix of feature values and a target variable\n",
    "y_hmeq = X_hmeq.pop('BAD')\n",
    "print('Loaded HMEQ credit risk data set with dimension (rows x columns) {} x {}'.format(*X_hmeq.shape))\n",
    "#--------------------------------------------------------\n",
    "# Training logistic regression-based classification model\n",
    "#--------------------------------------------------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression().fit(X_hmeq, y_hmeq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fe3e44-caaf-498d-855d-8a61711d0eb9",
   "metadata": {},
   "source": [
    "## Foundations of ML Model Evaluation\n",
    "As said, today we want to focus on prediction model evaluation. That said, an ML model has many facets, that deserve being evaluated. Examples include:\n",
    "- Interpretability\n",
    "- Robustness\n",
    "- Scalability\n",
    "- and many more\n",
    "\n",
    "Our focus, in this session is **predictive accuracy**. We will learn practices to rigorously assess the forecasts coming from a predictive models. Speaking about predictive accuracy...\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/main/resources/model_evaluation.png\" width=\"600\" height=\"600\" alt=\"Prediction Model Evaluation\">\n",
    "\n",
    "So, as discussed in the lecture, the evaluation of prediction performance is all about comparing model-based forecasts to actual outcomes. There are **two key ingredients** to make this comparison work. First, we must define what we mean by *performance*. For this, we need suitable accuracy or error measures. Second, we must organize our data suitably to simulate a real-world application of a model. Before diving into each of these stages, let's revisit how we obtain predictions in the first place. This is where the `predict()` function of `sklearn` comes in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31a5ca-6136-4f4d-9396-07412a4b595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo of how to obtain predictions from our regression model for the California Housing data\n",
    "yhat_calh = lin_reg.predict(X_calh)  # forecasts of the median house value (i.e. the target) for the TRAINING data\n",
    "\n",
    "# Visualize actual realizations of the target and the corresponding predictions by \n",
    "# putting them together in a data frame. \n",
    "df_Y_cf_Yhat = pd.DataFrame({'Median house value (Y)' : y_calh, 'Model prediction (Yhat)': yhat_calh})  \n",
    "df_Y_cf_Yhat.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e4dbae4-98b6-4911-9334-b3435febdc23",
   "metadata": {},
   "source": [
    "Equipped with this information, actual outcomes of the target variable and corresponding model predictions, we are ready to compute performance statistic. For example, the lecture introduced you to statistics like the *mean square error (MSE)* or *root mean square error (RMSE)*. Just to remind you, their mathematical definition was: \n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 ;\\qquad   \\text{RMSE} = \\sqrt{\\text{MSE}}  $$\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |Y_i - \\hat{Y}_i| $$\n",
    "\n",
    "$$\\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^n \\left|\\frac{Y_i - \\hat{Y}_i}{Y_i}\\right|  $$\r\n",
    "\n",
    "We could easily compute these error measures using `numpy. Here is an example for the first twos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db44f720-2cb0-41de-b781-0499a632fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = 1/len(yhat) * np.sum((y_calh - yhat_calh)**2)\n",
    "rmse = np.sqrt(mse)\n",
    "print('MSE of the regression model is {:.3f}'.format(mse))\n",
    "print('RMSE of the regression model is {:.3f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67260675-bd4f-49cc-ad70-44aa56202cdf",
   "metadata": {},
   "source": [
    "Alternatively, the `sklearn` library provides ready-to-use functions to calculate standard error measures for regression models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf17d80-efc8-4dca-bf52-807a42e3d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "mse  = mean_squared_error(y_calh, yhat_calh)\n",
    "mae  = mean_absolute_error(y_calh, yhat_calh)\n",
    "mape = mean_absolute_percentage_error(y_calh, yhat_calh)\n",
    "print('MSE of the regression model is {:.3f}'.format(mse))\n",
    "print('RMSE of the regression model is {:.3f}'.format(np.sqrt(mse)))\n",
    "print('MAE of the regression model is {:.3f}'.format(mae))\n",
    "print('MAPE of the regression model is {:.3f}'.format(mape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d3e1e-fb2b-4859-ace4-ce64ee140dee",
   "metadata": {},
   "source": [
    "#### Exercise 1: \n",
    "With the help of the above demo for the linear regression model, your task is to produce predictions for the logistic regression model. Recall from [practical #3](https://github.com/stefanlessmann/ESMT_IML/blob/main/notebooks/p3_classification.ipynb) that logistic regression can give you two types of outputs, discrete classifications of whether an applicant is a good or a bad risk and probability predictions, capturing the model-estimated probability that an applicant is a good or a bad risk. Make sure you fully understand how these are different. \n",
    "\n",
    "Your task is to compute both types of predictions for every credit applications in the HMEQ data set. Afterwards, visualize these predictions next to the true repayment status (i.e., target variable) by putting everything into a pandas data frame, as exemplified above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e02d6f-90ea-4ef3-9a05-b6de54576561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for exercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab028c-11c8-491b-93ad-73264f4e21db",
   "metadata": {},
   "source": [
    "## Data Organization\n",
    "At least for the case of regression, we have seen some examples of suitable - or at least common - measures of forecast accuracy. We will come back to this point later, when examining the performance of our logistic regression model. Before doing this, however, let's look at the other key ingredient to forecast accuracy evaluation, data organization. \n",
    "\n",
    "Above, when producing predictions and comparing these to actually observed outcomes, we used all available data. Technically speaking, we did that when executing codes like:\n",
    "```\n",
    "lin_reg.predict(X_calh)\n",
    "\n",
    "```\n",
    "Here, we call the `predict()` function and use all our data, which we store in variable `X_calh`, as argument. When training our regression model, we called the corresponding Python function in a similar way, namely:\n",
    "```\n",
    "lin_reg = LinearRegression().fit(X_calh, y_calh)\n",
    "```\n",
    "You can find both statements above in the notebook if unsure. The point is that we use **the same data**.  `X_calh` in this case, for model training and when assessing the model. This is bad practice. The lecture already gave an intuition why this is bad practice and future lecture sessions will further elaborate on this crucial point. For now, just accept it is bad practice. We already learned about a better way to organize the data using the **holdout method**. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/main/resources/holdout_method.png\" width=\"600\" height=\"600\" alt=\"Holdout Method\">\n",
    "\n",
    "To implement the holdout method in Python we typically use the `train_test_split()` function, which is available in the module `sklearn.model_selection`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e3158-b82d-4a3f-bd34-12a8cf99ebe9",
   "metadata": {},
   "source": [
    "#### Exercise 2: \n",
    "\n",
    "To familiarize yourself with the holdout methods, write some code to perform the following tasks\n",
    "- Import the `train_test_split()` function and have a look into its documentation\n",
    "- Use this insight to write code that calls the function and splits the California Housing data set, that is your variable `X_calh`, randomly into a **training set** and a **test set**. Your test set should comprise 30% of the total data.\n",
    "- Fit a linear regression model to the **training set**\n",
    "- Assess your regression model by computing the MAE on the **test set**\n",
    "- Also compute the MAE for the training set. Which error is larger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1824c52-be12-4cfb-8507-8c6c4c565417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for exercise 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b98314-bab2-4d99-906e-6c5ba1b7333c",
   "metadata": {},
   "source": [
    "#### Extra challenge (feel free to skip)\n",
    "You just compared the *test set performance* of the regression model to its *training set performance*. In theory, the error on the test set should be larger but it is well possible that you did not observe this behavior in the previous comparison. It is also possible that your previous comparison found both errors to be roughly the same. Well, so far your comparison has considered **one** random partitioning of the whole data into training and test set. To obtain a more reliable assessment of the training versus test error of the model, it would be useful to repeat the partitioning multiple times. Provided you already feel comfortable with coding, try to do this. More specifically, write code that implements the following logic:\n",
    "```\n",
    "SET parameter r, the number of repetition to some value > 100\n",
    "\n",
    "REPEAT r times\n",
    "    SPLIT California Housing data set randomly into training (80%) and test set (20%)\n",
    "    FIT linear regression model on the training set\n",
    "    PREDICT test set and compute MAE\n",
    "    PREDICT traiining set and compute MAE\n",
    "    STORE the MAE on training and on test set\n",
    "\n",
    "PLOT the distribution of the training set MAE and that of the test set MAE over the r repetitions\n",
    "```\n",
    "For the visualization part, you can use any plot you deem fit. Remember that previous practicals have illustrated several standard forms to visualize distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fcd946-3012-4792-b525-10157c036537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for the extra challenge - PLEASE SKIP IF YOU THINK THIS WOULD COST YOU A LOT OF TIME \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90a8b7cc-88a4-4091-b836-dc629a7b1ddd",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "Beyond splitting a data set into training and test set (aka the holdout method), we also introduced cross-validation, the process of partitioning a data set into *k* equally sized folds and repeatedly training a model on the combined data of k-1 folds and evaluating it on the left-out fold *k* times, such that each fold was used exactly once as the holdout or, synonymously , the *validation* fold. Relatively speaking, cross-validation is more reliable but also more costly than the simpler holdout method. It's costly because you have to train *k* models in total, which may cost a lot of time (e.g., when working with large data sets and/or complex ML algorithms). For the very same reason, cross-validation is also more reliable. You do not evaluate using one randomly sampled test set but carry out the evaluation *k* times.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/main/resources/cross_validation.png\" width=\"800\" height=\"600\" alt=\"Cross Validation\">\n",
    "\n",
    "\n",
    "The `sklearn` library offers you several ways to perform cross-validation. \n",
    "\r\n",
    "- `cross_val_sco()r is s a quick and easy wn, budoes not offer much flexibility. t it only returns a single metric, limiting its usefulness if you want to evaluate your model on multiple metrics.\r\n",
    "- `cross_valid()a It allows you to specify multiple evaluation metrics and provides more information about the training and testing procre`.\r\n",
    "- `K()F gIt gives full ore control over the cross-validation pro and is the most flexible implementation. ta. However, it requ you writing res more ccompared to han the other two metho\n",
    "So e.\r\n",
    "\r\n",
    "Each method has its own strengths and weaknesses, and the best choice depends on your specific  \n",
    "\n",
    "Below, we demonstrate the `cross_validate()` options, which should often strike a good balance between flexibility and simpli code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b377c99-c1cd-46ba-b7ca-422cb0cbbe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate, KFold\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Preliminaries\n",
    "#--------------------------------------------------------------------------------\n",
    "k = 10  # number of folds\n",
    "kf = KFold(n_splits=k)  # initialize cross-validation process\n",
    "\n",
    "# Say we want to assess our regression model in terms of multiple error\n",
    "# measures, just as we did before. Here we create a list of all the\n",
    "# error measures that interest us. \n",
    "error_measures = [mean_absolute_error,               # Note that the entries we put in this list   \n",
    "                  mean_squared_error,                # are actually the functions used before.\n",
    "                  mean_absolute_percentage_error]    # So we construct a list of Python functions\n",
    "err_shorthand = ['MAE', 'MSE', 'MAPE']\n",
    "\n",
    "# We will assess our regression model using each of the error.\n",
    "# We will also measure performance on the test and training set,\n",
    "# so we can compare, as before. To do all this, we need a place\n",
    "# in which we can store the results, that is:\n",
    "# k folds * 3 error measures * 2 sets (training and test).\n",
    "# We use dictionaries for this purpose and initialize one dictionary\n",
    "# for the test set results and another for the training set results. \n",
    "# In these dictionaries we use the name of a error measures as key, and \n",
    "# create an empty list to store the cross-validation results\n",
    "train_errors = {measure: [] for measure in err_shorthand}\n",
    "test_errors = {measure: [] for measure in err_shorthand}\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Iterating over the k folds\n",
    "#--------------------------------------------------------------------------------\n",
    "for train_index, test_index in kf.split(X_calh):  # so KFold gives as two arrays with the indices of the training and validation data of the current iteration\n",
    "    X_train, X_test = X_calh.iloc[train_index], X_calh.iloc[test_index]  # we can use these arrays to index our original data set: here we construct the feature matrices\n",
    "    y_train, y_test = y_calh[train_index], y_calh[test_index]  # and here we construct the arrays with the true targets\n",
    "\n",
    "    # Fitting linear regression model on the training set of THIS ITERATION\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    # Compute predictions on the test (and training) set of THIS ITERATION\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute model performance for every error measure we are interested in\n",
    "    for i, measure in enumerate(error_measures):\n",
    "        # Recall we created two dictionaries to store the results. First, we\n",
    "        # take the training set results dictionary. We can use the current\n",
    "        # error measure as key to index the dictionary. Doing so returns the \n",
    "        # value that the dictionary associates with this key. Given how we \n",
    "        # created the dictionary, this value is a list. Hence, we can use \n",
    "        # function .append() to add an element to the list. What value to append?\n",
    "        # The value of the error measure, which we calculate inside the below\n",
    "        # call of the append() function. \n",
    "        train_errors[err_shorthand[i]].append(measure(y_train, y_train_pred))\n",
    "        test_errors[err_shorthand[i]].append(measure(y_test, y_test_pred))\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Aggregating the results\n",
    "#--------------------------------------------------------------------------------\n",
    "# The main work is done. We are ready to report and/or visualize our results\n",
    "\n",
    "# For start, let's showcase the raw results for illustration \n",
    "print('The raw results per error measure are a list:')\n",
    "print(test_errors['MAE'])  # same approach for any other error measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a60d4f-b3ba-422a-976a-060089cf4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given our results have the form of lists, we could also report averages\n",
    "for i, measure in enumerate(error_measures):\n",
    "    e = err_shorthand[i]  # shorthand form simplify the code\n",
    "    print('\\nEvaluating results in terms of {}'.format(e))\n",
    "    print('-------------------------------------------')\n",
    "    print('\\tMEAN TEST set {} of the regression model is {:.3f}'.format(e, np.mean(test_errors[e])))\n",
    "    print('\\tMEAN TRAINING set {} of the regression model is {:.3f}'.format(e, np.mean(train_errors[e])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a422bb-30e9-4939-8c57-54d3cb6551dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally, we could create some suitable visualizations that depict how the error varies across folds\n",
    "# For example, we can produce 3 box plots, one for each error measure, and compare test to training error\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))  # Create a figure instance and specify the figure size\n",
    "\n",
    "for i, measure in enumerate(error_measures):\n",
    "    e = err_shorthand[i]  # shorthand form simplify the code\n",
    "    \n",
    "    # Create boxplot on each subplot\n",
    "    axs[i].boxplot([train_errors[e], test_errors[e]], labels=['Train', 'Test'])\n",
    "    axs[i].set_title(e)\n",
    "    \n",
    "axs[0].set_ylabel('Error')  # Just one y-axis label for the leftmost plot\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c41582-ecb7-40f6-b1c9-325c5fc21041",
   "metadata": {},
   "source": [
    "The coding demo was quite comprehensive. It is clearly time to give you a chance to try out your own coding skills. Prior to that, let's quickly note at least a few key findings from our analysis. \n",
    "\n",
    "First, the test error varies a lot more than does the training error. This is less surprising when you remember how cross-validation works. The test set, of each iteration, contains roughly $\\frac{1}{k}$ of the data while the training set is much bigger, containing all $\\frac{k-1}{k}$ remaining data.\n",
    "\n",
    "Second, in this case, the median of the training and test set error distributions is roughly identical. This finding may look counter intuitive because we motivated cross-validation by saying that the performance on the training set is not a reliable estimate. While this statement is true, the results we observe are still plausible because we used linear regression. It is a characteristic of linear regression that training and test error deviate less than in other learning algorithms, as we will see in future sessions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1431e2a9-1504-4483-a6b4-8b36c3214caf",
   "metadata": {},
   "source": [
    "### Exercise 3: \n",
    "Now that you saw the arguably most flexible and complex way to perform cross validation, your task is to simplify the above code using one of the other two options. This way, you will become familiar with all options and can make an informed choice next time you have to cross-validate a model. \n",
    "\n",
    "We continue with focusing on the California Housing data and linear regression. Depending on your preferences, we propose **two flavors of the exercise**. \n",
    "\n",
    "#### Option 3.a (easier)\n",
    "If you prefer an *easier go*, your task is to implement a 10-fold cross-validation of a regression model using the function `cross_val_score()`. More specifically:\n",
    "- Examine the function's documentation and make sure you understand its key arguments.\n",
    "- Write code to call the function such that it cross-validates a linear regression model for the California Housing data set\n",
    "- Perform ten fold cross-validation\n",
    "- Print the average performance of your model\n",
    "\n",
    "Should the above tasks have proved too easy we have a little extra task for you. Ask yourself what specific error measure your code has considered. It is not so clear, right? So your extra task could be to revise your solution such that it considers the MAE. \n",
    "\n",
    "<hr/>\n",
    "\n",
    "#### Option 3.b (harder)\n",
    "So this version is for the ambitious coders among you. It is similar to the above but involves using the function `cross_validate()`, multiple error measures, and a more sophisticated way to visualize the results. Specifically, \n",
    "- Examine the function's documentation and make sure you understand its key arguments\n",
    "- Write code to call the function such that it cross-validates a linear regression model for the California Housing data set\n",
    "- Perform ten fold cross-validation\n",
    "- Your evaluation should comprise different error measures, namely, the MAE, the MSE, and the MAPE.\n",
    "- Visualize the performance of the regression model for each error measure using a box-plot\n",
    "\n",
    "> **Hint**: take a look into the [sklearn documentation on the scoring parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#multimetric-scoring) to learn about ways of specifying error measures in a cross-validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc229a4c-bea7-4e87-b2ed-5f2be9fa54e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution to exercise 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ceed3c-3d25-4055-aa49-5c3d1035189a",
   "metadata": {},
   "source": [
    "## Measuring classification performance\n",
    "We cannot finish without discussing classification performance. The evaluation of a classifier differs from a regression model because our forecasting target is a categorical and often binary variable. We focus on the latter case and return to our HMEQ data set associated with predicting good and bad credit risks.\n",
    "\n",
    "### Confusion matrix to assess discrete class predictions\n",
    "Let's start with the more intuitive case in which we assess the binary class predictions of a logistic regression model. \n",
    "\n",
    "#### Exercise 4:\n",
    "We start with an easy task to get started: \n",
    "- Split the HMEQ data set into training (70%) and test set (30%) using the holdout method\n",
    "- Train a logistic regression model on the training set\n",
    "- Compute discrete class predictions using the test set\n",
    "- Store your predictions in a variable `class_pred`\n",
    "\n",
    "You can find pretty much all codes needed to solve this exercise in previous parts of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b65014-36dd-4c92-bf79-81a24fc6aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for exercise 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ed4b2-cbaf-4d54-8f7e-237529edf555",
   "metadata": {},
   "source": [
    "Well done! Having sorted this bit, let's move on with the actual assessment. We introduced the **confusion matrix** as a standard way to assess a binary classification models. Of course, `sklearn` also supports the confusion matrix and, more specifically, its visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840ee8d-b547-4a9c-b58f-36345f16bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "ConfusionMatrixDisplay.from_predictions(yts_hmeq,    # of course we use the test set\n",
    "                                        class_pred,  # here, we assume you solved exercise 4 and created an array of discrete class predictions\n",
    "                                        display_labels=['Good risk', 'Bad risk'])  # more readable labels for labeling the axes of our matrix\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e17ca-60df-4cf8-abd3-2ffe7638c0fe",
   "metadata": {},
   "source": [
    "Just for the records, in case you are only interested in the entries of the confusion matrix, an easy way to get these is  as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec1f253-c74d-47a3-b55e-78715c99dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = confusion_matrix(yts_hmeq, class_pred)  # Of course we use the test\n",
    "print(cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115d7a26-8a6d-4ebb-9592-c891ed491b7f",
   "metadata": {},
   "source": [
    "### ROC analysis\n",
    "The confusion matrix suggests that our logistic regression is doing a very poor. It consistently predicts the class *Good risk*, meaning it would give every applicant credit. Put in statistical terms, the logistic regression acts like a **naive classifier** by always predicting the majority class in the data.  \n",
    "\n",
    "Remember that the confusion matrix depicts the performance of a classifier for one specific **cut-off*. By default, `sklearn` and, more specifically, the `predict()` function, sets this cut-off to 0.5. It may be that our logistic regression is a bit more useful that we think. Specifically, it may be that the default cut-off was inappropriate and that we would see a better performance with a different cut-off. Entry **ROC-analysis**.\n",
    "\n",
    "ROC-analysis visualizes the performance of a classification model across **all possible cut-offs** and will give us a deeper understanding of whether the logistic regression is really naive or in fact able to distinguish good and bad payers to some extend. \n",
    "\n",
    "#### Exercise 5:\n",
    "Here is your task. \n",
    "- Familiarize yourself with the function `RocCurveDisplay` which is available in the `sklearn.metrics` module.\n",
    "- Write code to obtain **probability predictions** for the test set of the HMEQ data set (you can reuse `log_reg` for this task)\n",
    "- Using the functionality of `RocCurveDisplay`, create an ROC curve.\n",
    "- Write a short interpretation of the result. How well does logistic regression performs in your opinion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa7ea0-70fe-4b2b-9dac-831af4b19409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for exercise 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f0851-ec6d-46a4-8a3f-72cbe289d9e0",
   "metadata": {},
   "source": [
    "**Your interpretation of the ROC curve**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

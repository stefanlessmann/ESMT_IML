{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32daf08-0c21-4ea8-8c23-b76b4be5b806",
   "metadata": {},
   "source": [
    "# Practical 4: Prediction Model Evaluation\n",
    "<hr>\n",
    "We have learnt about linear and logistic regression. These methods facilitate supervised learning and enable us to approach prediction problems with a numerical (e.g., price) or categorical (e.g., repayment behavior=good or bad) target variable. \n",
    "\n",
    "In this session, we focus on practices to assess the accuracy of predictive models based on linear- or logistic regression. You will find more demos and small programming tasks. As to the tasks, remember that you are not supposed to know how to solve them right away. Rather, you are supposed to find solutions by discussing with peers, searching the web, or involving GenAI. That said, also recall that we never use GenAI without carefully inspecting generated codes **and** reading carefully the explanations of codes that are typically produced alongside generated codes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fc25b5-773f-47f3-a832-54fd0338dac4",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "We will use the two data sets introduced in previous sessions, the *California Housing* data set from [practical #2](https://github.com/stefanlessmann/ESMT_IML/blob/main/notebooks/p2_modeling_housing_prices.ipynb) and the *HMEQ* data set from [practical #3](https://github.com/stefanlessmann/ESMT_IML/blob/main/notebooks/p3_classification.ipynb), which represent, respectively, a regression associated with modeling median house prices at the district level and a classification problem associated with predicting the repayment behavior of credit applicants (aka credit risk modeling). Below we reproduce codes from earlier sessions to import standard libraries, load the data sets, and perform some rudimentary data preprocessing where needed. Just execute those codes and we are ready to go.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b49a3-f1ff-41fa-86d2-473b80a82bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Loading the California Housing data set\n",
    "#--------------------------------------------------------\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "calh = fetch_california_housing(as_frame=True)  # get the data as a Pandas dataframe\n",
    "# separate the data into feature matrix X and target variable y\n",
    "X_calh = calh.data\n",
    "y_calh = calh.target\n",
    "print('Loaded California Housing data set with dimension (rows x columns) {} x {}'.format(*X_calh.shape))\n",
    "#--------------------------------------------------------\n",
    "# Training (aka fitting) a linear regression model\n",
    "#--------------------------------------------------------\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression().fit(X_calh, y_calh)\n",
    "#--------------------------------------------------------\n",
    "# Loading the HMEQ credit risk data set\n",
    "#--------------------------------------------------------\n",
    "data_url = 'https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/master/data/hmeq.csv'\n",
    "hmeq = pd.read_csv(data_url)  # standard pandas function to load tabular data in CSV format\n",
    "\n",
    "# Convert a category with k different values into k-1 binary variables. \n",
    "X_hmeq = pd.get_dummies(hmeq, dummy_na=True, drop_first=True)\n",
    "X_hmeq = X_hmeq.dropna().reset_index(drop=True)  # drop all cases with one or more missing value\n",
    "\n",
    "# Separate the data into a matrix of feature values and a target variable\n",
    "y_hmeq = X_hmeq.pop('BAD')\n",
    "print('Loaded HMEQ credit risk data set with dimension (rows x columns) {} x {}'.format(*X_hmeq.shape))\n",
    "#--------------------------------------------------------\n",
    "# Training logistic regression-based classification model\n",
    "#--------------------------------------------------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression().fit(X_hmeq, y_hmeq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fe3e44-caaf-498d-855d-8a61711d0eb9",
   "metadata": {},
   "source": [
    "## Foundations of ML Model Evaluation\n",
    "As said, today we want to focus on prediction model evaluation. An ML model has many facets, that deserve being evaluated. Examples include:\n",
    "- Interpretability\n",
    "- Robustness\n",
    "- Scalability\n",
    "- and many more\n",
    "\n",
    "Our focus, in this session, is **predictive accuracy**. We will learn practices to rigorously assess the forecasts coming from a predictive models. Speaking about predictive accuracy, you might remember this crucial slide.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/main/resources/model_evaluation.png\" width=\"1280\" height=\"720\" alt=\"Prediction Model Evaluation\">\n",
    "\n",
    "So, as discussed in class, the evaluation of prediction performance is all about comparing model-based forecasts to actual outcomes. There are **two key ingredients** to make this work. First, we must define what we mean by *performance*. For this, we need suitable accuracy or error measures. Second, we must organize our data suitably to simulate a real-world application of a model. Before diving into each of these stages, let's revisit how we obtain predictions in the first place. This is where the `predict()` function of `sklearn` comes in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31a5ca-6136-4f4d-9396-07412a4b595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo of how to obtain predictions from our regression model for the California Housing data\n",
    "yhat_calh = lin_reg.predict(X_calh)  # forecasts of the median house value (i.e. the target) for the TRAINING data\n",
    "\n",
    "# Visualize actual realizations of the target and the corresponding predictions by \n",
    "# putting them together in a data frame. \n",
    "df_Y_cf_Yhat = pd.DataFrame({'Median house value (Y)' : y_calh, 'Model prediction (Yhat)': yhat_calh})  \n",
    "df_Y_cf_Yhat.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e4dbae4-98b6-4911-9334-b3435febdc23",
   "metadata": {},
   "source": [
    "Equipped with this information, actual outcomes of the target variable and corresponding model predictions, we are ready to compute performance statistic. For example, the lecture introduced you to statistics like the *mean square error (MSE)* or *root mean square error (RMSE)*. Just to remind you, their mathematical definition was: \n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 ;\\qquad   \\text{RMSE} = \\sqrt{\\text{MSE}}  $$\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |Y_i - \\hat{Y}_i| $$\n",
    "\n",
    "$$\\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^n \\left|\\frac{Y_i - \\hat{Y}_i}{Y_i}\\right|  $$\n",
    "\n",
    "We could easily compute these error measures using `numpy`. Here is an example for the first two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db44f720-2cb0-41de-b781-0499a632fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = 1/len(yhat) * np.sum((y_calh - yhat_calh)**2)\n",
    "rmse = np.sqrt(mse)\n",
    "print('MSE of the regression model is {:.3f}'.format(mse))\n",
    "print('RMSE of the regression model is {:.3f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67260675-bd4f-49cc-ad70-44aa56202cdf",
   "metadata": {},
   "source": [
    "Alternatively, the `sklearn` library provides ready-to-use functions to calculate standard error measures for regression models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf17d80-efc8-4dca-bf52-807a42e3d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "mse  = mean_squared_error(y_calh, yhat_calh)\n",
    "mae  = mean_absolute_error(y_calh, yhat_calh)\n",
    "mape = mean_absolute_percentage_error(y_calh, yhat_calh)\n",
    "print('MSE of the regression model is {:.3f}'.format(mse))\n",
    "print('RMSE of the regression model is {:.3f}'.format(np.sqrt(mse)))\n",
    "print('MAE of the regression model is {:.3f}'.format(mae))\n",
    "print('MAPE of the regression model is {:.3f}'.format(mape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d3e1e-fb2b-4859-ace4-ce64ee140dee",
   "metadata": {},
   "source": [
    "#### Exercise 1: \n",
    "With the help of the above demo for the linear regression model, your task is to produce predictions for the logistic regression model. Recall from [practical #3](https://github.com/stefanlessmann/ESMT_IML/blob/main/notebooks/p3_classification.ipynb) that logistic regression can give you two types of outputs, discrete classifications of whether an applicant is a good or a bad risk and probability predictions, capturing the model-estimated probability that an applicant is a good or a bad risk. Make sure you fully understand how these are different. \n",
    "\n",
    "Your task is to compute both types of predictions for every credit applications in the HMEQ data set. Afterwards, visualize these predictions next to the true repayment status (i.e., target variable) by putting everything into a pandas data frame, as exemplified above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e02d6f-90ea-4ef3-9a05-b6de54576561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to compute and visualize TRAINING set predictions for the logistic regression model\n",
    "class_pred = log_reg.predict(X_hmeq)\n",
    "prob_pred = log_reg.predict_proba(X_hmeq)\n",
    "pd.DataFrame({'Repayment status (1=default, 0=repaid)' : class_pred, \n",
    "              'Probability of default': prob_pred[:,1],\n",
    "              'Probability of repayment': prob_pred[:,0]\n",
    "             } \n",
    "            )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab028c-11c8-491b-93ad-73264f4e21db",
   "metadata": {},
   "source": [
    "## Data Organization\n",
    "At least for the case of regression, we have seen some common measures of forecast accuracy. We will come back to this point later, when examining the performance of our logistic regression model. Before doing this, however, let's look at the other key ingredient to forecast accuracy evaluation, data organization. \n",
    "\n",
    "Above, when producing predictions and comparing these to actually observed outcomes, we used all available data. Technically speaking, we did that when executing codes like:\n",
    "```\n",
    "lin_reg.predict(X_calh)\n",
    "\n",
    "```\n",
    "Here, we call the `predict()` function and use all our data, which we store in variable `X_calh`, as argument. When training our regression model, we called the corresponding Python function in a similar way, namely:\n",
    "```\n",
    "lin_reg = LinearRegression().fit(X_calh, y_calh)\n",
    "```\n",
    "You can find both statements above in the notebook if unsure. The point is that we use **the same data**, `X_calh`, for model training **and** when assessing the model. This is bad practice. The lecture already clarified data organization and future lecture sessions will further elaborate on this crucial point. We also learned about a better way to organize the data using the **holdout method**. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/main/resources/holdout_method.png\" width=\"600\" height=\"600\" alt=\"Holdout Method\">\n",
    "\n",
    "To implement the holdout method in Python we typically use the `train_test_split()` function, which is available in the module `sklearn.model_selection`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e3158-b82d-4a3f-bd34-12a8cf99ebe9",
   "metadata": {},
   "source": [
    "#### Exercise 2: \n",
    "\n",
    "To familiarize yourself with the holdout methods, write code to perform the following tasks\n",
    "- Import the `train_test_split()` function and have a look into its documentation\n",
    "- Use this insight to write code that calls the function and splits the California Housing data set, that is your variable `X_calh`, randomly into a **training set** and a **test set**. Your test set should comprise 30% of the total data.\n",
    "- Fit a linear regression model to the **training set**\n",
    "- Assess your regression model by computing the MAE on the **test set**\n",
    "- Also compute the MAE for the training set. Which error is larger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1824c52-be12-4cfb-8507-8c6c4c565417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import train_test_split() and call the function to split your data into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr_calh, Xts_calh, ytr_calh, yts_calh = train_test_split(X_calh, y_calh, test_size=0.3, random_state=111)\n",
    "lin_reg = LinearRegression().fit(Xtr_calh, ytr_calh)\n",
    "print('TEST set MAE of the regression model is {:.3f}'.format(\n",
    "    mean_absolute_error(yts_calh, lin_reg.predict(Xts_calh))\n",
    "))\n",
    "print('TRAINING set MAE of the regression model is {:.3f}'.format(\n",
    "    mean_absolute_error(ytr_calh, lin_reg.predict(Xtr_calh))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b98314-bab2-4d99-906e-6c5ba1b7333c",
   "metadata": {},
   "source": [
    "#### Extra challenge (Python beginners should skip this part)\n",
    "You just compared the *test set performance* of the regression model to its *training set performance*. In theory, the error on the test set should be larger but it is possible that you did not observe this behavior. Perhaps, your previous comparison found both errors to be roughly the same. \n",
    "\n",
    "Note that your comparison has considered **one** random partitioning of the whole data into training and test set. To obtain a more reliable assessment of the training versus test error of the model, it would be useful to repeat the partitioning. If you already feel comfortable with coding, try to implement this better way of comparing the errors. More specifically, write code that implements the following logic:\n",
    "```\n",
    "SET parameter r, the number of repetition to some value > 100\n",
    "\n",
    "REPEAT r times\n",
    "    SPLIT California Housing data set randomly into training (80%) and test set (20%)\n",
    "    FIT linear regression model on the training set\n",
    "    PREDICT test set and compute MAE\n",
    "    PREDICT training set and compute MAE\n",
    "    STORE the MAE on training and on test set\n",
    "\n",
    "PLOT the distribution of the training set MAE and that of the test set MAE over the r repetitions\n",
    "```\n",
    "For the visualization part, you can use any plot you deem fit. Remember that previous practicals have illustrated several standard forms to visualize distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fcd946-3012-4792-b525-10157c036537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for the extra challenge - PLEASE SKIP IF YOU THINK THIS WOULD COST YOU A LOT OF TIME \n",
    "mae_tr = []  # empty list to store training set MAE\n",
    "mae_ts = []  # empty list to store test set MAE\n",
    "\n",
    "r = 10000  # number of repetitions\n",
    "for i in np.arange(r):\n",
    "    if (i % int(r*0.1)==0) and i>0:  # print status every ten percent\n",
    "        print('Fitted {} regression models'.format(i))\n",
    "    Xtr_calh, Xts_calh, ytr_calh, yts_calh = train_test_split(X_calh, y_calh, test_size=0.2)\n",
    "    lin_reg = LinearRegression().fit(Xtr_calh, ytr_calh)\n",
    "    mae_tr.append(mean_absolute_error(ytr_calh, lin_reg.predict(Xtr_calh)))\n",
    "    mae_ts.append(mean_absolute_error(yts_calh, lin_reg.predict(Xts_calh)))\n",
    "\n",
    "\n",
    "print('MEAN TEST set MAE of the regression model is {:.3f}'.format(np.mean(mae_ts)))\n",
    "print('MEAN TRAINING set MAE of the regression model is {:.3f}'.format(np.mean(mae_tr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b937aecc-e03f-4aac-bcd0-c104da00e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))  \n",
    "axes = axes.flatten() # Flatten the axes for easier iteration\n",
    "\n",
    "axes[0].hist(mae_tr, bins=30, edgecolor='k',  label='MAE on Train')\n",
    "axes[0].hist(mae_ts, bins=30, edgecolor='k',  alpha=0.5, label='MAE on Test')\n",
    "axes[0].set_xlabel('MAE')\n",
    "axes[0].set_ylabel('Counts')\n",
    "axes[0].set_title('Distribution of test set MAE over {} runs'.format(r))\n",
    "axes[0].legend(loc='best')\n",
    "\n",
    "axes[1].boxplot([mae_ts, mae_tr], labels=['Test', 'Train'])\n",
    "axes[1].set_ylabel('MAE')\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb71697-bcf9-468b-a178-61d5a491dc24",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "Beyond splitting a data set into training and test set (aka the holdout method), we also introduced cross-validation, the process of partitioning a data set into *k* equally sized folds and repeatedly training a model on the combined data of k-1 folds and evaluating it on the left-out fold *k* times, such that each fold was used exactly once as the holdout or, synonymously , the *validation* fold. Relatively speaking, cross-validation is more reliable but also more costly than the simpler holdout method. It's costly because you have to train *k* models in total, which may cost a lot of time (e.g., when working with large data sets and/or complex ML algorithms). For the very same reason, cross-validation is also more reliable. You do not evaluate using one randomly sampled test set but carry out the evaluation *k* times.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/main/resources/cross_validation.png\" width=\"800\" height=\"600\" alt=\"Cross Validation\">\n",
    "\n",
    "\n",
    "The `sklearn` library offers you several ways to perform cross-validation. Each method has its own strengths and weaknesses, and the best choice depends on your specific requirements.\n",
    "- `cross_val_score()` is easy to use but does not offer much flexibility. \n",
    "- `cross_validate()`  allows you to specify multiple evaluation metrics and provides more information about the training and testing procedure`.\n",
    "- `KFold()` gives you full control over the cross-validation process. It is the most flexible implementation but requires writing more code compared to the other two methods.\n",
    "\n",
    "\n",
    "Below, we demonstrate the `KFold()` option. Take a little time to review the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b377c99-c1cd-46ba-b7ca-422cb0cbbe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate, KFold\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Preliminaries\n",
    "#--------------------------------------------------------------------------------\n",
    "k = 10  # number of folds\n",
    "kf = KFold(n_splits=k)  # initialize cross-validation process\n",
    "\n",
    "# Say we want to assess our regression model in terms of multiple error\n",
    "# measures, just as we did before. Here we create a list of all the\n",
    "# error measures that interest us. \n",
    "error_measures = [mean_absolute_error,               # Note that the entries we put in this list   \n",
    "                  mean_squared_error,                # are actually the functions used before.\n",
    "                  mean_absolute_percentage_error]    # So we construct a list of Python functions\n",
    "err_shorthand = ['MAE', 'MSE', 'MAPE']\n",
    "\n",
    "# We will assess our regression model using each of the error.\n",
    "# We will also measure performance on the test and training set,\n",
    "# so we can compare, as before. To do all this, we need a place\n",
    "# in which we can store the results, that is:\n",
    "# k folds * 3 error measures * 2 sets (training and test).\n",
    "# We use dictionaries for this purpose and initialize one dictionary\n",
    "# for the test set results and another for the training set results. \n",
    "# In these dictionaries we use the name of a error measures as key, and \n",
    "# create an empty list to store the cross-validation results\n",
    "train_errors = {measure: [] for measure in err_shorthand}\n",
    "test_errors = {measure: [] for measure in err_shorthand}\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Iterating over the k folds\n",
    "#--------------------------------------------------------------------------------\n",
    "for train_index, test_index in kf.split(X_calh):  # so KFold gives as two arrays with the indices of the training and validation data of the current iteration\n",
    "    X_train, X_test = X_calh.iloc[train_index], X_calh.iloc[test_index]  # we can use these arrays to index our original data set: here we construct the feature matrices\n",
    "    y_train, y_test = y_calh[train_index], y_calh[test_index]  # and here we construct the arrays with the true targets\n",
    "\n",
    "    # Fitting linear regression model on the training set of THIS ITERATION\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    # Compute predictions on the test (and training) set of THIS ITERATION\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute model performance for every error measure we are interested in\n",
    "    for i, measure in enumerate(error_measures):\n",
    "        # Recall we created two dictionaries to store the results. First, we\n",
    "        # take the training set results dictionary. We can use the current\n",
    "        # error measure as key to index the dictionary. Doing so returns the \n",
    "        # value that the dictionary associates with this key. Given how we \n",
    "        # created the dictionary, this value is a list. Hence, we can use \n",
    "        # function .append() to add an element to the list. What value to append?\n",
    "        # The value of the error measure, which we calculate inside the below\n",
    "        # call of the append() function. \n",
    "        train_errors[err_shorthand[i]].append(measure(y_train, y_train_pred))\n",
    "        test_errors[err_shorthand[i]].append(measure(y_test, y_test_pred))\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Aggregating the results\n",
    "#--------------------------------------------------------------------------------\n",
    "# The main work is done. We are ready to report and/or visualize our results\n",
    "\n",
    "# For start, let's showcase the raw results for illustration \n",
    "print('The raw results per error measure are a list:')\n",
    "print(test_errors['MAE'])  # same approach for any other error measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a60d4f-b3ba-422a-976a-060089cf4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given our results have the form of lists, we could also report averages\n",
    "for i, measure in enumerate(error_measures):\n",
    "    e = err_shorthand[i]  # shorthand form simplify the code\n",
    "    print('\\nEvaluating results in terms of {}'.format(e))\n",
    "    print('-------------------------------------------')\n",
    "    print('\\tMEAN TEST set {} of the regression model is {:.3f}'.format(e, np.mean(test_errors[e])))\n",
    "    print('\\tMEAN TRAINING set {} of the regression model is {:.3f}'.format(e, np.mean(train_errors[e])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a422bb-30e9-4939-8c57-54d3cb6551dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally, we could create some suitable visualizations that depict how the error varies across folds\n",
    "# For example, we can produce 3 box plots, one for each error measure, and compare test to training error\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))  # Create a figure instance and specify the figure size\n",
    "\n",
    "for i, measure in enumerate(error_measures):\n",
    "    e = err_shorthand[i]  # shorthand form simplify the code\n",
    "    \n",
    "    # Create boxplot on each subplot\n",
    "    axs[i].boxplot([train_errors[e], test_errors[e]], labels=['Train', 'Test'])\n",
    "    axs[i].set_title(e)\n",
    "    \n",
    "axs[0].set_ylabel('Error')  # Just one y-axis label for the leftmost plot\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c41582-ecb7-40f6-b1c9-325c5fc21041",
   "metadata": {},
   "source": [
    "The coding demo was quite comprehensive. It is clearly time to give you a chance to try out your own coding skills. Prior to that, let's quickly note at least a few key findings from our analysis. \n",
    "\n",
    "First, the test error varies a lot more than does the training error. This is less surprising when you remember how cross-validation works. The test set, of each iteration, contains roughly $\\frac{1}{k}$ of the data while the training set is much bigger, containing all $\\frac{k-1}{k}$ remaining data.\n",
    "\n",
    "Second, in this case, the median of the training and test set error distributions is roughly identical. This finding may look counter intuitive because we motivated cross-validation by saying that the performance on the training set is not a reliable estimate. While this statement is true, the results we observe are still plausible because we used linear regression. It is a characteristic of linear regression that training and test error deviate less than in other learning algorithms, as we will see in future sessions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1431e2a9-1504-4483-a6b4-8b36c3214caf",
   "metadata": {},
   "source": [
    "### Exercise 3: \n",
    "Now that you saw the arguably most flexible and complex way to perform cross validation, your task is to simplify the above code using one of the other two options. This way, you will become familiar with all options and can make an informed choice next time you have to cross-validate a model. \n",
    "\n",
    "We continue with focusing on the California Housing data and linear regression. Depending on your preferences, we propose **two flavors of the exercise**. \n",
    "\n",
    "#### Option 3.a (easier)\n",
    "Your task is to implement a 10-fold cross-validation of a regression model using the function `cross_val_score()`. More specifically:\n",
    "- Examine the function's documentation and make sure you understand its key arguments.\n",
    "- Write code to call the function such that it cross-validates a linear regression model for the California Housing data set\n",
    "- Perform ten fold cross-validation\n",
    "- Print the average performance of your model\n",
    "\n",
    "Should the above tasks have proved too easy we have a little extra task for you: ask yourself what specific error measure your code has considered. It is not so clear, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution to exercise 3 - easy way\n",
    "k=10  # no of folds\n",
    "\n",
    "cv_error = cross_val_score(LinearRegression(), X_calh, y_calh)\n",
    "print('Mean cross-validation error is {:.3f}'.format(np.mean(cv_error)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb44d3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<hr/>\n",
    "\n",
    "#### Option 3.b (harder)\n",
    "This version is similar to the above but involves the function `cross_validate()`, multiple error measures, and a more sophisticated way to visualize the results. Specifically, \n",
    "- Examine the function's documentation and make sure you understand its key arguments\n",
    "- Write code to call the function such that it cross-validates a linear regression model for the California Housing data set\n",
    "- Perform ten fold cross-validation\n",
    "- Your evaluation should comprise different error measures, namely, the MAE, the MSE, and the MAPE.\n",
    "- Visualize the performance of the regression model for each error measure using a box-plot\n",
    "\n",
    "> **Hint**: take a look into the [sklearn documentation on the scoring parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#multimetric-scoring) to learn about ways of specifying error measures in a cross-validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5b48325c-5a7c-4873-aac8-a5140701444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution to exercise 3 - hard way\n",
    "k=10  # no of folds\n",
    "\n",
    "# This is how we can specify the error measures we are interested in; see https://scikit-learn.org/stable/modules/model_evaluation.html#multimetric-scoring for further information\n",
    "error_measures = ['neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_absolute_percentage_error']\n",
    "\n",
    "cv_results = cross_validate(LinearRegression(),  # we use linear regression\n",
    "                            X_calh, y_calh,  # our data\n",
    "                            cv=k,  # number of folds\n",
    "                            scoring=error_measures,  # specify the error measures\n",
    "                            return_train_score=True  # also compute errors on the training set\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346c50cd-f446-4d1d-a5aa-8624456bd83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to visualize the results\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))  # Create a figure instance and specify the figure size\n",
    "\n",
    "# Keys to extract relevant data from the cv_results dictionary, which\n",
    "# the function cross_validate produced\n",
    "error_metrics = ['neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_absolute_percentage_error']\n",
    "titles = ['MAE', 'MSE', 'MAPE']  # More telling labels for the error measures\n",
    "\n",
    "# Produce 3 box plots, one for each error measure\n",
    "for i, metric in enumerate(error_metrics):\n",
    "    # Multiply by -1 to make errors positive\n",
    "    train_errors = cv_results['train_' + metric] * -1  # Have a look into the dictionary cv_results.\n",
    "    test_errors = cv_results['test_' + metric] * -1    # It uses keys like 'train_neg_mean_squared_error'  \n",
    "\n",
    "    # Create boxplot on each subplot\n",
    "    axs[i].boxplot([train_errors, test_errors], labels=['Train', 'Test'])\n",
    "    axs[i].set_title(titles[i])\n",
    "    \n",
    "axs[0].set_ylabel('Error')  # Just one y-axis label for the leftmost plot\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ceed3c-3d25-4055-aa49-5c3d1035189a",
   "metadata": {},
   "source": [
    "## Measuring classification performance\n",
    "We cannot finish without discussing classification performance. The evaluation of a classifier differs from a regression model because our forecasting target is a categorical and often binary variable. We focus on the latter case and return to our HMEQ data set associated with predicting good and bad credit risks.\n",
    "\n",
    "### Confusion matrix to assess discrete class predictions\n",
    "Let's start with the more intuitive case in which we assess the binary class predictions of a logistic regression model. \n",
    "\n",
    "#### Exercise 4:\n",
    "We start with an easy task to get started: \n",
    "- Split the HMEQ data set into training (70%) and test set (30%) using the holdout method\n",
    "- Train a logistic regression model on the training set\n",
    "- Compute discrete class predictions using the test set\n",
    "- Store your predictions in a variable `class_pred`\n",
    "\n",
    "You can find pretty much all codes needed to solve this exercise in previous parts of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b65014-36dd-4c92-bf79-81a24fc6aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for exercise 4\n",
    "Xtr_hmeq, Xts_hmeq, ytr_hmeq, yts_hmeq = train_test_split(X_hmeq, y_hmeq, test_size=0.3)\n",
    "log_reg = LogisticRegression().fit(Xtr_hmeq, ytr_hmeq)\n",
    "class_pred = log_reg.predict(Xts_hmeq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ed4b2-cbaf-4d54-8f7e-237529edf555",
   "metadata": {},
   "source": [
    "Well done! Having sorted this bit, let's move on with the actual assessment. We introduced the **confusion matrix** as a standard way to assess a binary classification models. Of course, `sklearn` also supports the confusion matrix and, more specifically, its visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840ee8d-b547-4a9c-b58f-36345f16bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "ConfusionMatrixDisplay.from_predictions(yts_hmeq,    # of course we use the test set\n",
    "                                        class_pred,  # here, we assume you solved exercise 4 and created an array of discrete class predictions\n",
    "                                        display_labels=['Good risk', 'Bad risk'])  # more readable labels for labeling the axes of our matrix\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e17ca-60df-4cf8-abd3-2ffe7638c0fe",
   "metadata": {},
   "source": [
    "Just for the records, in case you are only interested in the entries of the confusion matrix, an easy way to get these is  as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec1f253-c74d-47a3-b55e-78715c99dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = confusion_matrix(yts_hmeq, class_pred)  \n",
    "print(cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55064fdc",
   "metadata": {},
   "source": [
    "Note that it can be a bit confusing to interpret the numbers without seeing the labels of rows and columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115d7a26-8a6d-4ebb-9592-c891ed491b7f",
   "metadata": {},
   "source": [
    "### ROC analysis\n",
    "The confusion matrix suggests that our logistic regression is doing a very poor. It consistently predicts the class *Good risk*, meaning it would give every applicant credit. Put in statistical terms, the logistic regression acts like a **naive classifier** by always predicting the majority class in the data.  \n",
    "\n",
    "Remember that the confusion matrix depicts the performance of a classifier for one specific **cut-off**. By default, the `predict()` function of `sklearn` sets this cut-off to 0.5. It may be that our logistic regression is a bit more useful that we think. Specifically, it may be that the default cut-off was inappropriate and that we would see a better performance with a different cut-off. Entry **ROC-analysis**.\n",
    "\n",
    "ROC-analysis visualizes the performance of a classification model across **all possible cut-offs** and will give us a deeper understanding of whether the logistic regression is really naive or in fact able to distinguish good and bad payers to some extend. \n",
    "\n",
    "#### Exercise 5:\n",
    "- Familiarize yourself with the function `RocCurveDisplay` which is available in the `sklearn.metrics` module.\n",
    "- Write code to obtain **probability predictions** for the test set of the HMEQ data set (you can reuse `log_reg` for this task)\n",
    "- Using the functionality of `RocCurveDisplay`, create an ROC curve.\n",
    "- Write a short interpretation of the result. How well does logistic regression performs in your opinion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa7ea0-70fe-4b2b-9dac-831af4b19409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for exercise 5\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "prob_pred =  log_reg.predict_proba(Xts_hmeq)[:,1]\n",
    "RocCurveDisplay.from_predictions(yts_hmeq,  # true repayment behavior for test set observations\n",
    "                                 prob_pred, # probability predictions of the logistic regression model\n",
    "                                 name='Log. Regression',  # just formatting the legend of the plot\n",
    "                                 pos_label=1)\n",
    "plt.plot([0, 1], [0, 1], \"r--\")  # adding in the random benchmark, \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f0851-ec6d-46a4-8a3f-72cbe289d9e0",
   "metadata": {},
   "source": [
    "**Your interpretation of the ROC curve:**\n",
    "An AUC statistic of ~0.63 does still not suggest great performance but shows that the logistic regression is not as useless as the confusion matrix may have indicated. The ROC curve is above the random baseline (red dotted line) showing how, across all cut-offs, the probability predictions of logistic regression discriminate between good and bad payers to some extent. However, the ROC curve is still far from optimal. In practice, a risk scorecard with AUC less than, e.g., 0.8, would never make it into production.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e150b8",
   "metadata": {},
   "source": [
    "# Well done! You completed another comprehensive notebook full of concepts, demos and programming tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

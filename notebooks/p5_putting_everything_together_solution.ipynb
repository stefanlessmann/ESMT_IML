{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9894dd2-c88c-4157-8443-edae9dfe1e5c",
   "metadata": {},
   "source": [
    "# Practical 5: Putting it all together\n",
    "<hr>\n",
    "By now, you have seen several Python demos and progressed your coding abilities substantially. Therefore, it is safe and suitable for the fifth practical to focus more on exercises. This is what we will do. Past lecture sessions introduced relevant machine learning concepts, including tree-based models, neural networks, and the fundamental problem of overfitting. A lot of content to revisit. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67a76e-9f61-4821-828f-080f9c173b95",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "First of all, as usual, we import a couple of libraries. Also, we reproduce codes from previous sessions to prepare for subsequent tasks. In a nutshell, the following codes\n",
    "- load the *California Housing Data Set*,\n",
    "- partition the data set into a training (70%) and test set (30%)\n",
    "- fit and assess a linear regression model on the training and test set, respectively\n",
    "\n",
    "For the latter part, we introduce a custom function called `my_model_performance()` that takes care of the evaluation. Re-using this function will save you some time in Exercise 1. So take a little time to check out the function's code and how to use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a7f9e-2980-4bed-b2c2-71f7b6cf282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# SKlearn imports (all functions have been introduced in previous sessions)\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Loading the California Housing data set\n",
    "#--------------------------------------------------------\n",
    "calh = fetch_california_housing(as_frame=True)  # get the data as a Pandas dataframe\n",
    "# separate the data into feature matrix X and target variable y\n",
    "X = calh.data\n",
    "y = calh.target\n",
    "print('Loaded California Housing data set with dimension (rows x columns) {} x {}'.format(*X.shape))\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Data organization\n",
    "#--------------------------------------------------------\n",
    "test_frac = 0.3\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y, test_size=test_frac)\n",
    "print('{:.0f}% test set has dimension (rows x columns) {} x {}'.format(test_frac*100, *Xts.shape))\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Training (aka fitting) a linear regression model\n",
    "#--------------------------------------------------------\n",
    "lreg = LinearRegression().fit(Xtr, ytr)\n",
    "print('Estimated coefficients of linear regression model')\n",
    "df_print = pd.DataFrame(data=lreg.coef_, index=X.columns, columns=['Coefficient'])\n",
    "print(df_print)\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Predict the test set and compute prediction performance\n",
    "#--------------------------------------------------------\n",
    "yhat_lreg = lreg.predict(Xts)\n",
    "\n",
    "def my_model_performance(yhat, ytrue):\n",
    "    ''' Helper function to assess the accuracy\n",
    "        of forecasts in terms of MSE and MAE\n",
    "        and format the results for printing.\n",
    "\n",
    "        Inputs: yhat         (float) array with predictions\n",
    "                ytrue        (float) array with observed outcomes\n",
    "\n",
    "        Output: String; depicting model performance\n",
    "    '''\n",
    "\n",
    "    result = 'MSE={:.3f}, MAE={:.3f}'.format(\n",
    "        mean_squared_error(y_true=yts, y_pred=yhat), \n",
    "        mean_absolute_error(y_true=yts, y_pred=yhat)\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "print('Performance of linear regression model:\\t', \n",
    "      my_model_performance(yhat_lreg, ytrue=yts)\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d92917-593e-45a1-acd1-8bceaa1abafd",
   "metadata": {},
   "source": [
    "## Exercise 1: Warm-Up\n",
    "Since the last coding sessions, we have heard about *Decision Trees* and *Neural Networks* but never really used them. Hence, this is what we do next. Here is your task: \n",
    "\n",
    "You can find the functionality to use the above learners in the modules `sklearn.neural_network` and  `sklearn.tree`. The corresponding classes are called, respectively, `MLPRegressor` and `DecisionTreeRegressor`. This information should suffice to train and assess your first neural network and tree model. More specifically, use the training data `Xtr` and `ytr`, which we created in the *Preliminaries* for training one network and one tree. Next, compute test set predictions and assess the models in the same way as illustrated above for linear regression using our custom function `my_model_performance()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3702ed17-49ed-4bd7-95da-a0a7966f6f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution to Exercise 1\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Model training\n",
    "dtree = DecisionTreeRegressor().fit(Xtr, ytr)\n",
    "nnet = MLPRegressor().fit(Xtr, ytr)\n",
    "\n",
    "# Model evaluation using our customer helper function\n",
    "print('Performance of linear regression model:\\t', \n",
    "      my_model_performance(lreg.predict(Xts), ytrue=yts)\n",
    "     )\n",
    "print('Performance of neural network model:\\t', \n",
    "      my_model_performance(nnet.predict(Xts), ytrue=yts)\n",
    "     )\n",
    "print('Performance of decision tree model:\\t', \n",
    "      my_model_performance(dtree.predict(Xts), ytrue=yts)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed212516-42bd-4b00-bd98-a8540950d3c1",
   "metadata": {},
   "source": [
    "Take a little time to examine model performance. Which model performs the best on this data set, the decision tree or the neural network? And do not forget to also compare to linear regression. There is no guarantee that more advanced learning algorithms give better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2636756-f40f-4343-9a3c-cb6a14da383c",
   "metadata": {},
   "source": [
    "## Exercise 2: Overfitting\n",
    "How did the lecture introduce overfitting? As the **fundamental problem** in machine learning. If that does not sound important, right? In theory, the decision tree and neural network model, which you just created should both be vulnerable to overfitting. However, we recommend you use the decision tree for Exercise #2 because it is faster to train. Here is your task:\n",
    "\n",
    "Overfitting is a problem of complex machine learning models. The complexity of a decision tree depends, amongst others, on its depth. Explore the documentation of `DecisionTreeRegressor` and familiarize yourself with the argument `max_depth`. As the name suggests, this argument allows you to create decision tree models with different depths (i.e., number of splits). Train and assess multiple trees of increasing depth. Start with a depth of one and increase it incrementally. For each candidate depth, grow a tree of that depth and compute the corresponding test set error. Also compute the training set error and store these quantities. Then, create a plot that depicts how both errors, training and test error, develop when the depth of the tree increases. Based on the lecture, you should have a good idea of how the resulting plot should look like... \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stefanlessmann/ESMT_IML/main/resources/bias_var_overfitting.png\" width=\"1280\" height=\"720\" alt=\"Bias, variance, and the fundamental problem of overfitting\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952ec071-e752-4297-9ea3-325f7de85dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution to Exercise 2\n",
    "mdepths = range(1, 25)\n",
    "mdepths[0]\n",
    "\n",
    "mse_tr = []\n",
    "mse_ts = []\n",
    "for _, d in enumerate(mdepths):\n",
    "    dtree = DecisionTreeRegressor(max_depth=d).fit(Xtr, ytr)\n",
    "    mse_tr.append( mean_squared_error(ytr, dtree.predict(Xtr)) )\n",
    "    mse_ts.append( mean_squared_error(yts, dtree.predict(Xts)) )\n",
    "\n",
    "plt.plot(mdepths, mse_tr, label=['Training error'])\n",
    "plt.plot(mdepths, mse_ts, label=['Test error'])\n",
    "plt.xlabel('Maximum tree depth')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45cc29f-4051-4c6b-ab5c-33eb97eea68f",
   "metadata": {},
   "source": [
    "At this point, you should look at an enticing visualization of the **overfitting problem**. The deeper the decision tree the more complex the model. More splits imply that the tree can model more complicated relationships between the features and the target. However, this ability can result in the tree fitting the *training* data too well. Once the tree starts to capture idiosyncratic noise during training and transfers this noise to its predictions, the error on the test set should increase. Therefore, your visualization highlights two important points. It provides you with empirical proof that overfitting is a real problem that you must be aware of. Further, and more practically, the plot can also suggest a suitable tree depth (i.e., meta-parameter setting) for this data set. The point where the test set error is the lowest marks the optimal depth for the tree for this data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a03e01-c884-4947-be5c-3007ade298c2",
   "metadata": {},
   "source": [
    "## Exercise 3: Benchmarking\n",
    "Given that many different learning algorithms with different merits and demerits exist, and given that none of these should be considered generally better than others, a common scenario in machine learning is that one aims to find the most suitable approach for a given data set. Well, we are given a data set, right? So let's try to mimic this *benchmarking* scenario. To make it more interesting, we can further extend the set of learning algorithms we consider. The lecture has, although only briefly, mentioned two common tree-based ensemble algorithms, Random Forest and Gradient Boosting. You can easily access those via `sklearn`, which refers to them as `RandomForestRegressor` and `GradientBoostingRegressor`. These classes are available in the module `sklearn.ensemble`. \n",
    "\n",
    "For this exercise, you do not need to write much new code. Virtually all codes that you need have already appeared in one way or the other in previous parts of the notebook. Therefore, this exercise provides a great opportunity to further advance your coding skills. The idea of a benchmark is that you assess multiple learning algorithms. Each one undergoes a standardized treatment: train, compute predictions, compute performance, store results, etc. Surely there is a way to write the code for benchmarking several alternative learning algorithms in a suitable way without too much copy and pasting ;)\n",
    "\n",
    "Here is your task:<be>\n",
    "Implement a benchmarking experiment in which you assess the test set performance the following learning algorithms:\n",
    "- Linear Regression\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- XGB\n",
    "- Neural Network\n",
    "\n",
    "Find out which one works best for the California Housing data set. To that end, plot the test set MAE of each model using a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea99d00-787a-495a-aea7-3ed102a43c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "# Note that the configuration of some learning algorithms is chosen to ensure reasonably fast training\n",
    "models = [LinearRegression(),\n",
    "          MLPRegressor(hidden_layer_sizes=(5,)), \n",
    "          DecisionTreeRegressor(max_depth=8),  # based on Excercise 2, a max depth of 8 seems suitable\n",
    "          RandomForestRegressor(n_estimators=5, max_depth=8), \n",
    "          GradientBoostingRegressor(n_estimators=10)]\n",
    "model_keys = ['LR', 'MLP', 'DT', 'RF', 'GBM']  # these keys are mainly introduced for plotting and reporting results\n",
    "\n",
    "dict_yhat = {}  # empty dictionary to store prediction\n",
    "dict_mae = {}  # empty dictionary to store results in terms of MAE\n",
    "\n",
    "# Loop over the learning algorithms\n",
    "for i,m in enumerate(models):\n",
    "    print('Fit model {}...'.format(m))  # letting the caller know where we are\n",
    "    m.fit(Xtr, ytr)  # train model\n",
    "    dict_yhat[model_keys[i]] = m.predict(Xts)  # predict test set and store the forecasts\n",
    "    dict_mae[model_keys[i]] = mean_absolute_error(yts, dict_yhat[model_keys[i]])  # compute MAE and store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db18cf-8ae3-44e5-98fd-4fd682e9a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bar plot\n",
    "plt.barh(list(dict_mae.keys()), list(dict_mae.values()))\n",
    "plt.xlabel('MAE')\n",
    "plt.title('Benchmarking results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ef7d3-f03b-4057-8b15-f55bfce10e86",
   "metadata": {},
   "source": [
    "## Exercise 4 (Extra): Hyperparameter tuning\n",
    "Solving Exercise 3 is a major achievement. Several academic papers have been published on the basis of such benchmarks; so you may be proud to have made it up to this point!\n",
    "\n",
    "In this additional exercise, we revisit some of the learning algorithms from the previous comparison and check whether we gave them fair treatment. More specifically, advanced learning algorithms like the neural network or gradient boosting exhibit several hyperparameters. For example, the number of boosting iterations is a hyperparameter in gradient boosting. Likewise, the architecture of the neural network needs to be determined by the data scientist. The fact that we did not specify any architecture above or did not specify the number of boosting iterations implies that all hyperparameters were set to default values. This approach is not appropriate. One should tune the hyperparameters of an algorithm to adjust it to a given data set. The `sklearn` library offers functionality for this task in the module `sklearn.model_selection`. This exercise is to familiarize yourself with some of the corresponding options for hyperparameter tuning.\n",
    "\n",
    "Here is your task:\n",
    "- Import the the function `GridSearchCV` from the module `sklearn.model_selection` and study its documentation.\n",
    "- Consider the gradient boosting regressor.\n",
    "    - Create a dictionary for tuning three of its hyperparameters, the number of base model trees (`n_estimators`), the maximum depth of individual decision trees (`max_depth`), and a third hyperparameter called the learning rate (`learning_rate`).\n",
    "    - The following code snipped illustrates the structure of such dictionaries. We use the name of the hyperparameters (as shown above) as keys and set the corresponding values equal to lists of candidate settings for the hyperparameter. You can simply copy and paste the code into your solution.\n",
    "- Call the `GridSearchCV` methods with the following specification:\n",
    "    - Set the argument `param_grid` equal to your dictionary of candidate hyperparameter settings\n",
    "    - Set the argument `estimator` equal to `GradientBoostingRegressor()`\n",
    "    - Set the argument `cv` equal to 3\n",
    "    - Set the argument `verbose` equal to 2\n",
    "    - Make sure to store the result of the function by writing something like `searcher = GridSearchCV(....`\n",
    "- Start the search for optimal hyperparameters by calling the well-known `fit()` function for the GridSearchCV object. For example, continuing the previous demo, your code could look like this: `searcher.fit()`.\n",
    "- Note that running the `fit()` function may take a while. Once completed, inspect the result and determine which hyperparameters gave the best gradient boosting model, that is, delivered the lowest forecasting error.\n",
    "- Compute test set predictions and their MAE using the best hyperparameter values. \n",
    "- Did hyperparameter tuning improve gradient boosting? Is the tuned model competitive to the learners of the Exercise 3 benchmark? \n",
    "\n",
    "**Demo of how to specify a search grid for hyperparameter tuning:** \n",
    "```\n",
    "hyperparameter = {\n",
    "    'n_estimators': [25, 50, 100],\n",
    "    'max_depth': [2, 3, 5]\n",
    "    'learning_rate' : [0.1, 0.01]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b809731-1fa0-45c2-8d5a-5c8c780c553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "?GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8621899-ddbf-4801-a665-7f82491b0982",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter = {\n",
    "    'n_estimators': [25, 25, 100],\n",
    "    'max_depth': [2, 3, 5],\n",
    "    'learning_rate' : [0.1, 0.01]\n",
    "}\n",
    "\n",
    "gbm = GridSearchCV(GradientBoostingRegressor(), \n",
    "                   param_grid=hyperparameter,\n",
    "                   cv=3,\n",
    "                   verbose=2\n",
    "                  ).fit(Xtr, ytr)\n",
    "gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18cf79c-4ebf-4a61-bc79-5a9200746ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Performace of untuned GBM:\\t', my_model_performance(ytrue=yts, yhat=dict_yhat['GBM']))\n",
    "print('Performace of tuned GBM:\\t', my_model_performance(ytrue=yts, yhat=gbm.predict(Xts)))\n",
    "print('Performace of best benchmark:\\t', my_model_performance(ytrue=yts, yhat=dict_yhat['RF'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7fd438-b892-47fb-978d-28beff9e6c76",
   "metadata": {},
   "source": [
    "I assume your result shows that tuning has improved gradient boosting substantially and that the tuned boosting model outperforms all competitors with quite some margin. At least, this is the result I obtained when solving the tasks and running the notebook. After all, there is a reason for many data scientists calling gradient boosting *the best off the shelf learner*. Next time you seek a powerful learning algorithm for a regression or classification problem involving tabular data, pick gradient boosting, tune its hyperparameter to your data, this should give you a reasonably good idea of how well the target variable in your data can be predicted based on the available features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8781028e",
   "metadata": {},
   "source": [
    "\n",
    "# Excellent Work! This was a challenging notebook, and you made it all through!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
